{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №6\n",
    "# Деревья решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Примеры заданий итогового теста\n",
    "\n",
    "_Дан размеченный набор данных с числовыми признаками с метками классов. При помощи метода $K$ ближайших соседей найти прогнозируемый класс для заданной точки, используя в качестве расстояния:_\n",
    "\n",
    "* _евклидову метрику_\n",
    "$$\\sqrt{\\sum_{i}\\left|x_{i}-y_{i}\\right|^{2}}$$\n",
    "\n",
    "* _манхэттенское расстояние_\n",
    "$$\\sum_{i}\\left|x_{i}-y_{i}\\right|$$\n",
    "\n",
    "* _метрику Чебышева_\n",
    "$$\\max_{i}\\left|x_{i}-y_{i}\\right|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание__: Дан размеченный набор данных с двумя числовыми признаками и метками классов 1 и 2 (третья компонента):\n",
    "\n",
    "$$D=\\left\\{ \\left(0,0,1\\right),\\left(2,0,1\\right),\\left(0,2,1\\right),\\left(1,1,2\\right),\\left(3,2,2\\right)\\right\\}$$ \n",
    "\n",
    "При помощи метода $K$ ближайших соседей для $K=3$ найти прогнозируемый класс для заданной точки $x=\\left(1.5,0.5\\right)$, используя в качестве расстояния евклидову метрику."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Решение__: Вычислим расстояния от точки $x$ до точек набора данных $D$:\n",
    "\n",
    "$$\\rho\\left(x,x_{1}\\right)=\\sqrt{\\left(1.5-0\\right)^{2}+\\left(0.5-0\\right)^{2}}=0.5\\,\\sqrt{10}$$\n",
    "$$\\rho\\left(x,x_{2}\\right)=\\sqrt{\\left(1.5-2\\right)^{2}+\\left(0.5-0\\right)^{2}}=0.5\\,\\sqrt{2}$$\n",
    "$$\\rho\\left(x,x_{3}\\right)=\\sqrt{\\left(1.5-0\\right)^{2}+\\left(0.5-2\\right)^{2}}=0.5\\,\\sqrt{18}$$\n",
    "$$\\rho\\left(x,x_{4}\\right)=\\sqrt{\\left(1.5-1\\right)^{2}+\\left(0.5-1\\right)^{2}}=0.5\\,\\sqrt{2}$$\n",
    "$$\\rho\\left(x,x_{5}\\right)=\\sqrt{\\left(1.5-3\\right)^{2}+\\left(0.5-2\\right)^{2}}=0.5\\,\\sqrt{18}$$\n",
    "\n",
    "Три ближайших точки – это $x_{1}$, $x_{2}$ и $x_{4}$, поэтому прогнозируемый класс – это $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Дан набор данных с двумя числовыми признаками с истинными и предсказанными метками классов (третья и четвертая компоненты)._\n",
    "\n",
    "_Матрица ошибок классификатора состоит из элементов:_\n",
    "\n",
    "$$n_{ij}=\\left|\\left\\{ \\overline{\\mathbf{x}}_{l}\\in\\mathbf{D}\\mid\\hat{y}_{l}=c_{i},\\,y_{l}=c_{j}\\right\\} \\right|$$\n",
    "\n",
    "_Вычислить:_ \n",
    "\n",
    "* _долю ошибок (error rate) классификатора_\n",
    "$$\\text{Error rate}=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{\\mathbf{I}}\\left(y_{i}\\neq\\hat{y}_{i}\\right)$$\n",
    "\n",
    "* _долю верных ответов (accuracy) классификатора_   \n",
    "$$\\text{Accuracy}=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{\\mathbf{I}}\\left(y_{i}=\\hat{y}_{i}\\right)=1-\\text{Error rate}$$\n",
    "\n",
    "* _точность (accuracy, precision) класса_\n",
    "$$acc_{i}=\\frac{n_{ii}}{m_{i}}$$\n",
    "\n",
    "*\t_полнота (recall) класса_    \n",
    "$$recall_{i}=\\frac{n_{ii}}{n_{i}}$$\n",
    "\n",
    "*\t_F-меру класса_  \n",
    "$$F_{i}=\\frac{2}{\\frac{1}{acc_{i}}+\\frac{1}{recall_{i}}}=\\frac{2\\,acc_{i}\\,recall_{i}}{acc_{i}+recall_{i}}=\\frac{2\\,n_{ii}}{n_{i}+m_{i}}$$\n",
    "\n",
    "*\t_общую F-меру классификатора_  \n",
    "$$F=\\frac{1}{k}\\sum_{i=1}^{k}F_{i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание__: Дан набор данных с двумя числовыми признаками (первые две компоненты) и истинными и предсказанными метками классов (третья и четвертая компоненты): \n",
    "\n",
    "$$D=\\left\\{ (0,0,1,1),(1,0,1,1),(1,1,1,2),(0,1,2,2),(2,0,2,1),(2,1,2,2),(2,2,1,1),(1,2,2,2),(0,2,2,2)\\right\\}$$ \n",
    "\n",
    "Вычислить долю ошибок (error rate) классификатора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Решение__: \n",
    "\n",
    "Доля ошибок (error rate) классификатора вычисляется по формуле\n",
    "\n",
    "$$\\text{Error rate}=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{\\mathbf{I}}\\left(y_{i}\\neq\\hat{y}_{i}\\right),$$\n",
    "\n",
    "поэтому\n",
    "$$\\text{Error rate}=\\frac{1}{9}\\left(0+0+1+0+1+0+0+0+0\\right)=\\frac{2}{9}=0.222$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Дана матрица ошибок бинарной классификации_ \n",
    "$$\n",
    "\\left(\\begin{array}{cc}\n",
    "TP & FN\\\\\n",
    "FP & TN\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "_Вычислить:_\n",
    "\n",
    "* _долю ошибок (error rate) классификатора_ \n",
    "$$Error rate=\\frac{FP+FN}{n}$$\n",
    "\n",
    "* _долю верных ответов (accuracy) классификатора_\n",
    "$$Accuracy=\\frac{TP+TN}{n}$$\n",
    "\n",
    "* _точность (precision) для положительного класса_ \n",
    "$$prec_{P}=\\frac{TP}{TP+FP}=\\frac{TP}{m_{1}}$$\n",
    "\n",
    "* _точность (precision) для отрицательного класса_ \n",
    "$$prec_{N}=\\frac{TN}{TN+FN}=\\frac{TN}{m_{2}}$$\n",
    "\n",
    "* _долю корректно спрогнозированных положительных точек (TPR, sensitivity)_ \n",
    "$$TPR=recall_{P}=\\frac{TP}{TP+FN}=\\frac{TP}{n_{1}}$$\n",
    "\n",
    "* _долю корректно спрогнозированных отрицательных точек (TNR, specificity)_ \n",
    "$$TNR=recall_{N}=\\frac{TN}{FP+TN}=\\frac{TN}{n_{2}}$$\n",
    "\n",
    "* _долю ошибочно спрогнозированных отрицательных точек (FNR)_ $$FNR=\\frac{FN}{TP+FN}=\\frac{FN}{n_{1}}=1-sensitivity$$\n",
    "\n",
    "* _долю ошибочно спрогнозированных положительных точек (FPR)_ $$FPR=\\frac{FP}{FP+TN}=\\frac{FP}{n_{2}}=1-specificity$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание__: Дана матрица ошибок бинарной классификации: \n",
    "\n",
    "$$\\left(\\begin{array}{cc}\n",
    "TP=8+G & FN=2\\\\\n",
    "FP=3 & TN=7+G\n",
    "\\end{array}\\right),$$\n",
    "\n",
    "где $G$ – параметр, принимающий целое положительное значение. \n",
    "\n",
    "Вычислить долю верных ответов (accuracy) классификатора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Решение__:  Доля верных ответов (accuracy) классификатора вычисляется по формуле\n",
    "\n",
    "$$Accuracy=\\frac{TP+TN}{n},$$\n",
    "\n",
    "поэтому \n",
    "$$n=TP+FP+FN+TN=8+G+3+2+7+G=20+2\\,G,$$\n",
    "$$Accuracy=\\frac{8+G+7+G}{20+2\\,G}=\\frac{15+2\\,G}{20+2\\,G}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Деревья (принятия) решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дерево принятия решений представляет собой иерархическую древовидную структуру, состоящую из решающих правил вида «Если ..., то ...». Правила автоматически генерируются в процессе обучения на обучающем множестве. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем набор \"Ирисы\" (3 и 4 признаки):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:,2:]\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем набор на плоскости:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1])\n",
    "plt.scatter(X[y==2,0], X[y==2,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для построения дерева решений будем использовать класс `DecisionTreeClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)  \n",
    "dt_clf.fit(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим границу принятия решения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, axis):\n",
    "    \n",
    "    x0, x1 = np.meshgrid(\n",
    "        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),\n",
    "        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),\n",
    "    )\n",
    "    X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "    y_predict = model.predict(X_new)\n",
    "    zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])\n",
    "    \n",
    "    plt.contourf(x0, x1, zz, cmap=custom_cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(dt_clf, axis=[0.5, 7.5, 0, 3])\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1])\n",
    "plt.scatter(X[y==2,0], X[y==2,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем дерево принятия решений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plot_tree(dt_clf); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 9))\n",
    "plot_tree(dt_clf,filled=True); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Энтропия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Энтропия измеряет размеры беспорядка или неопределенности в системе. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    return (-p * np.log(p) - (1-p) * np.log(1-p))/np.log(2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.01, 0.99, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, entropy(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка разбиения при помощи энтропии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиение имеет более низкую энтропию (или низкий беспорядок), если оно относительно чисто, т.е. большинство точек имеют одну и ту же метку. С другой стороны, разбиение имеет более высокую энтропию (или беспорядок), если метки классов перемешаны и не существует определенного класса для большинства меток.\n",
    "\n",
    "Если область чиста, т.е. состоит из точек одного и того же класса, то энтропия равна нулю. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = DecisionTreeClassifier(max_depth=3, criterion=\"entropy\", random_state=42)\n",
    "dt_clf.fit(X, y)\n",
    "plot_decision_boundary(dt_clf, axis=[0.5, 7.5, 0, 3])\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1])\n",
    "plt.scatter(X[y==2,0], X[y==2,1])\n",
    "plt.show()\n",
    "plot_tree(dt_clf); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение (точка разбиения) $X_{d}\\leqslant v$ разбивает пространство данных $\\mathbf{D}$ на две области $\\mathbf{D}_{Y}$ и $\\mathbf{D}_{N}$, которые представляют собой множества всех возможных точек, которые удовлетворяют решению, и тех точек, которые ему не удовлетворяют. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(X, y, d, v):\n",
    "    index_a = (X[:,d] <= v)\n",
    "    index_b = (X[:,d] > v)\n",
    "    return X[index_a], X[index_b], y[index_a], y[index_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dy,Dn,_,_ = split(X, y, 0, 1.4)\n",
    "Dy[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Энтропия множества помеченных точек $\\mathbf{D}$ определяется как\n",
    "\n",
    "$H\\left(\\mathbf{D}\\right)=-\\sum_{i=1}^{k}\\mathbb{P}\\left[c_{i}\\mid\\mathbf{D}\\right]\\,\\log_{2}\\mathbb{P}\\left[c_{i}\\mid\\mathbf{D}\\right],$\n",
    "\n",
    "где $\\mathbb{P}\\left[c_{i}\\mid\\mathbf{D}\\right]$ – вероятность класса $c_{i}$ в $\\mathbf{D}$ и $k$ – число классов.\n",
    "\n",
    "\n",
    "Если классы перемешаны и каждый появляется с равной вероятностью $\\mathbb{P}\\left[c_{i}\\mid\\mathbf{D}\\right]=\\frac{1}{k}$, то энтропия имеет наивысшее значение $H\\left(\\mathbf{D}\\right)=\\log_{2}k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "def entropy(y):\n",
    "    counter = Counter(y)\n",
    "    res = 0.0\n",
    "    for num in counter.values():\n",
    "        p = num / len(y)\n",
    "        res += -p * log(p)\n",
    "    return res/log(2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим энтропию разбиения (split entropy) как взвешенную энтропию каждой из образующихся областей \n",
    "\n",
    "$H\\left(\\mathbf{D}_{Y},\\mathbf{D}_{N}\\right)=\\frac{n_{Y}}{n}H\\left(\\mathbf{D}_{Y}\\right)+\\frac{n_{N}}{n}H\\left(\\mathbf{D}_{N}\\right),$\n",
    "\n",
    "где $n=\\left|\\mathbf{D}\\right|$ – это число точек в $\\mathbf{D}$ и $n_{Y}=\\left|\\mathbf{D}_{Y}\\right|$ и $n_{N}=\\left|\\mathbf{D}_{N}\\right|$ – это число точек в $\\mathbf{D}_{Y}$ и $\\mathbf{D}_{N}$. \n",
    "\n",
    "Для определения параметров оптимального разбиения (включая энтропию) используем следующую функцию: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_split_ent(X, y):\n",
    "    \n",
    "    best_entropy = float('inf')\n",
    "    best_d, best_v = -1, -1\n",
    "    for d in range(X.shape[1]):\n",
    "        sorted_index = np.argsort(X[:,d])\n",
    "        for i in range(1, len(X)):\n",
    "            if X[sorted_index[i], d] != X[sorted_index[i-1], d]:\n",
    "                v = (X[sorted_index[i], d] + X[sorted_index[i-1], d])/2\n",
    "                X_l, X_r, y_l, y_r = split(X, y, d, v)\n",
    "                p_l, p_r = len(X_l) / len(X), len(X_r) / len(X)\n",
    "                e = p_l * entropy(y_l) + p_r * entropy(y_r)\n",
    "                if e < best_entropy:\n",
    "                    best_entropy, best_d, best_v = e, d, v\n",
    "                \n",
    "    return best_entropy, best_d, best_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_entropy, best_d, best_v = try_split_ent(X, y)\n",
    "print(\"Лучшая энтропия разбиения =\", best_entropy)\n",
    "print(\"Лучший признак (d) =\", best_d)\n",
    "print(\"Лучшая точка (v) =\", best_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_l, X1_r, y1_l, y1_r = split(X, y, best_d, best_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(y1_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(y1_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_entropy2, best_d2, best_v2 = try_split_ent(X1_r, y1_r)\n",
    "print(\"Лучшая энтропия разбиения =\", best_entropy2)\n",
    "print(\"Лучший признак (d) =\", best_d2)\n",
    "print(\"Лучшая точка (v) =\", best_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_l, X2_r, y2_l, y2_r = split(X1_r, y1_r, best_d2, best_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(y2_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(y2_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Индекс Джини (Gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Индекс Джини (для множества) определяется следующим образом:\n",
    "\n",
    "$G\\left(\\mathbf{D}\\right)=1-\\sum_{i=1}^{k}\\mathbb{P}\\left[c_{i}\\mid\\mathbf{D}\\right]^{2}$\n",
    "\n",
    "Если разбиение чисто, то индекс Джини равен нулю.\n",
    "\n",
    "Взвешенный индекс Джини разбиения равен\n",
    "\n",
    "$G\\left(\\mathbf{D}_{Y},\\mathbf{D}_{N}\\right)=\\frac{n_{Y}}{n}G\\left(\\mathbf{D}_{Y}\\right)+\\frac{n_{N}}{n}G\\left(\\mathbf{D}_{N}\\right)$\n",
    "\n",
    "Чем меньше значение взвешенного индекса Джини, тем лучше разбиение. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = DecisionTreeClassifier(max_depth=3, criterion=\"gini\", random_state=42)\n",
    "dt_clf.fit(X, y)\n",
    "plot_decision_boundary(dt_clf, axis=[0.5, 7.5, 0, 3])\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1])\n",
    "plt.scatter(X[y==2,0], X[y==2,1])\n",
    "plt.show()\n",
    "plot_tree(dt_clf); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(y):\n",
    "    counter = Counter(y)\n",
    "    res = 1.0\n",
    "    for num in counter.values():\n",
    "        p = num / len(y)\n",
    "        res -= p**2\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_split_gini(X, y):\n",
    "    \n",
    "    best_g = float('inf')\n",
    "    best_d, best_v = -1, -1\n",
    "    for d in range(X.shape[1]):\n",
    "        sorted_index = np.argsort(X[:,d])\n",
    "        for i in range(1, len(X)):\n",
    "            if X[sorted_index[i], d] != X[sorted_index[i-1], d]:\n",
    "                v = (X[sorted_index[i], d] + X[sorted_index[i-1], d])/2\n",
    "                X_l, X_r, y_l, y_r = split(X, y, d, v)\n",
    "                p_l, p_r = len(X_l) / len(X), len(X_r) / len(X)\n",
    "                g = p_l * gini(y_l) + p_r * gini(y_r)\n",
    "                if g < best_g:\n",
    "                    best_g, best_d, best_v = g, d, v\n",
    "                \n",
    "    return best_g, best_d, best_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_g, best_d, best_v = try_split_gini(X, y)\n",
    "print(\"Лучший взвешенный индекс Джини =\", best_g)\n",
    "print(\"Лучший признак (d) =\", best_d)\n",
    "print(\"Лучшая точка (v) =\", best_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_l, X1_r, y1_l, y1_r = split(X, y, best_d, best_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini(y1_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini(y1_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_g2, best_d2, best_v2 = try_split_gini(X1_r, y1_r)\n",
    "print(\"Лучший взвешенный индекс Джини =\", best_g2)\n",
    "print(\"Лучший признак (d) =\", best_d2)\n",
    "print(\"Лучшая точка (v) =\", best_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_l, X2_r, y2_l, y2_r = split(X1_r, y1_r, best_d2, best_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini(y2_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini(y2_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Параметры конструктора класса DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конструктор класса `DecisionTreeClassifier` имеет целый ряд параметров, включая следующие:\n",
    "\n",
    "* criterion{“gini”, “entropy”}, default=”gini”\n",
    "  * функция для измерения качества разбиения\n",
    "\n",
    "* splitter{“best”, “random”}, default=”best”\n",
    "  * стратегия разбиения \n",
    "\n",
    "* max_depth: int, default=None\n",
    "  * максимальная глубина дерева. Если None, то узлы раскрываются до тех пор, пока все листья не будут чистыми или пока все листья не будут содержать меньше, чем min_samples_split точек\n",
    "\n",
    "* min_samples_split: int or float, default=2\n",
    "  * минимальное количество точек для разбиения внутреннего узла\n",
    "\n",
    "* min_samples_leaf: int or float, default=1\n",
    "  * минимальное количество точек, необходимое на узле для разбиения\n",
    "\n",
    "* max_leaf_nodes: int, default=None\n",
    "  * максимальное количество узлов-листьев \n",
    "  \n",
    "Эти и другие параметры существенно влияют на работу классификатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "X, y = datasets.make_moons(noise=0.25, random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(X, y)\n",
    "\n",
    "plot_decision_boundary(dt_clf, axis=[-1.5, 2.5, -1.0, 1.5])\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plot_tree(dt_clf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf2 = DecisionTreeClassifier(max_depth=3)\n",
    "dt_clf2.fit(X, y)\n",
    "\n",
    "plot_decision_boundary(dt_clf2, axis=[-1.5, 2.5, -1.0, 1.5])\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1])\n",
    "plt.show()\n",
    "plot_tree(dt_clf2); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf3 = DecisionTreeClassifier(min_samples_split=10)\n",
    "dt_clf3.fit(X, y)\n",
    "\n",
    "plot_decision_boundary(dt_clf3, axis=[-1.5, 2.5, -1.0, 1.5])\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1])\n",
    "plt.show()\n",
    "plot_tree(dt_clf3); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf4 = DecisionTreeClassifier(min_samples_leaf=6)\n",
    "dt_clf4.fit(X, y)\n",
    "\n",
    "plot_decision_boundary(dt_clf4, axis=[-1.5, 2.5, -1.0, 1.5])\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1])\n",
    "plt.show()\n",
    "plot_tree(dt_clf4); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf5 = DecisionTreeClassifier(max_leaf_nodes=8)\n",
    "dt_clf5.fit(X, y)\n",
    "\n",
    "plot_decision_boundary(dt_clf5, axis=[-1.5, 2.5, -1.0, 1.5])\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1])\n",
    "plt.show()\n",
    "plot_tree(dt_clf5); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проблемы при использовании деревьев решений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data[:,2:]\n",
    "y = iris.target\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(max_depth=2, criterion=\"entropy\", random_state=1)\n",
    "tree_clf.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(tree_clf, axis=[0.5, 7.5, 0, 3])\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1])\n",
    "plt.scatter(X[y==2,0], X[y==2,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.delete(X, 140, axis=0)\n",
    "y_new = np.delete(y, 140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf2 = DecisionTreeClassifier(max_depth=2, criterion=\"entropy\", random_state=42)\n",
    "tree_clf2.fit(X_new, y_new);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(tree_clf2, axis=[0.5, 7.5, 0, 3])\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1])\n",
    "plt.scatter(X[y==2,0], X[y==2,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ансамбли классификаторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ансамблевые методы (ensemble methods) создают комбинированный классификатор, используя выходные данные нескольких базовых классификаторов, которые обучаются на разных наборах данных.  \n",
    "В зависимости от выбора обучающих наборов и стабильности базовых классификаторов ансамблевые классификаторы могут помочь уменьшить дисперсию и смещение, что приведет к повышению качества классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons(n_samples=500, noise=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "log_clf.fit(X_train, y_train)\n",
    "log_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC()\n",
    "svm_clf.fit(X_train, y_train)\n",
    "svm_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(max_depth=2,random_state=666)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "dt_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict1 = log_clf.predict(X_test)\n",
    "y_predict2 = svm_clf.predict(X_test)\n",
    "y_predict3 = dt_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим комбинированный бинарный классификатор следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.array((y_predict1 + y_predict2 + y_predict3) >= 2, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построенный комбинированный классификатор имеет более высокое качество классификации, чем любой из использованных базовых классификаторов.\n",
    "\n",
    "Для построения комбинированных классификаторов в scikit-learn есть класс `VotingClassifier`, позволяющий объединить  несколько не похожих между собой классификаторов в один классификатор.\n",
    "\n",
    "Среди параметров `VotingClassifier` есть параметр `voting` с двумя возможными значениями: 'hard' и 'soft'. \n",
    "В первом случае итоговый ответ объединенного классификатора будет соответствовать «мнению» большинства входящих в него классификаторов. Во втором случае, т.е. при использовании значения 'soft' параметра  `voting` идет полноценное «голосование» и взвешивание предсказаний моделей для каждого класса, таким образом итоговый ответ объединенного классификатор — это `argmax` суммы предсказанных вероятностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('log_clf', LogisticRegression()), \n",
    "    ('svm_clf', SVC()),\n",
    "    ('dt_clf', DecisionTreeClassifier(random_state=666))],\n",
    "                             voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf.fit(X_train, y_train)\n",
    "voting_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При использовании класса `VotingClassifier` с параметром 'soft' получаем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf2 = VotingClassifier(estimators=[\n",
    "    ('log_clf', LogisticRegression()), \n",
    "    ('svm_clf', SVC(probability=True)),\n",
    "    ('dt_clf', DecisionTreeClassifier(random_state=666))],\n",
    "                             voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf2.fit(X_train, y_train)\n",
    "voting_clf2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм построения стекированного классификатора реализован в классе `StackingClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "stacking_clf = StackingClassifier(estimators=[\n",
    "    ('log_clf', LogisticRegression()), \n",
    "    ('svm_clf', SVC(probability=True)),\n",
    "    ('dt_clf', DecisionTreeClassifier(random_state=666))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_clf.fit(X_train, y_train)\n",
    "stacking_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод бэггинга"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод бэггинга (Bagging, Bootstrap Aggregation) представляет собой ансамблевый метод классификации, который использует несколько выборок (с заменой) из входного набора данных для создания немного различающихся обучающих наборов.\n",
    "Термин `Bootstrap` означает, что используются имеющиеся данные и они размножаются при помощи выборок с заменой — в результате получаются несколько наборов одного размера и немного отличающихся друг от друга. \n",
    "Бэггинг состоит из следующих шагов:\n",
    "1. Получение нескольких образцов с помощью замены из вашего набора данных (бутстрэпинг).\n",
    "2. Обучение классификатора для каждого образца.\n",
    "3. Усреднение результатов каждого классификатора.\n",
    "\n",
    "Основная идея бэггинга — усреднить «шумные» модели с низким смещением для создания модели с низкой дисперсией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons(n_samples=500, noise=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagging_clf = BaggingClassifier(DecisionTreeClassifier(),\n",
    "                           n_estimators=50, max_samples=100,\n",
    "                           bootstrap=True)\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "bagging_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_clf = BaggingClassifier(DecisionTreeClassifier(),\n",
    "                           n_estimators=5000, max_samples=100,\n",
    "                           bootstrap=True)\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "bagging_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Процедура OOB (out-of-bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерация повторных случайных выборок из исходного набора данных (бутстреп) дает хорошую возможность провести специальную процедуру перекрестной проверки, называемую тестом по наблюдениям, \"не попавшим в сумку\" (out-of-bag observations). Поскольку ключевая идея бэггинга состоит в многократном построении моделей по наблюдениям из бутстреп-выборок, то каждое конкретное дерево строится на основе примерно двух третей всех наблюдений. Остальная треть наблюдений не используется в обучении, но вполне может быть использована для независимого тестирования.\n",
    "\n",
    "OOB-ошибка — это встроенная версия расчёта ошибки классификатора. Это удобно, потому что  не нужно откладывать часть точек в тестовую выборку в самом начале. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_clf = BaggingClassifier(DecisionTreeClassifier(),\n",
    "                               n_estimators=50, max_samples=100,\n",
    "                               bootstrap=True, oob_score=True)\n",
    "bagging_clf.fit(X, y)\n",
    "bagging_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для ускорения обучения классификатора на многоядерных процессорах можно использовать параметр `n_jobs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bagging_clf = BaggingClassifier(DecisionTreeClassifier(),\n",
    "                               n_estimators=5000, max_samples=100,\n",
    "                               bootstrap=True, oob_score=True)\n",
    "bagging_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bagging_clf = BaggingClassifier(DecisionTreeClassifier(),\n",
    "                               n_estimators=5000, max_samples=100,\n",
    "                               bootstrap=True, oob_score=True,\n",
    "                               n_jobs=-1)\n",
    "bagging_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уменьшение размера выборки может приводить к повышению качества классификатора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_subspaces_clf = BaggingClassifier(DecisionTreeClassifier(),\n",
    "                               n_estimators=500, max_samples=500,\n",
    "                               bootstrap=True, oob_score=True,\n",
    "                               max_features=1, bootstrap_features=True)\n",
    "random_subspaces_clf.fit(X, y)\n",
    "random_subspaces_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_patches_clf = BaggingClassifier(DecisionTreeClassifier(),\n",
    "                               n_estimators=500, max_samples=100,\n",
    "                               bootstrap=True, oob_score=True,\n",
    "                               max_features=1, bootstrap_features=True)\n",
    "random_patches_clf.fit(X, y)\n",
    "random_patches_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Случайный лес и сверхслучайные деревья"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод случайного леса (Random Forest) представляет собой дальнейшее улучшение бэггинга деревьев решений, которое заключается в устранении корреляции между деревьями. Как и в случае с бэггингом, мы строим несколько сотен деревьев решений по обучающим бутстреп-выборкам. Однако на каждой итерации построения дерева случайным образом выбирается некоторое число признаков и разбиение разрешается выполнять только по одному из этих признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons(n_samples=500, noise=0.3, random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=500, oob_score=True, \n",
    "                                random_state=666, n_jobs=-1)\n",
    "rf_clf.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf2 = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, oob_score=True, \n",
    "                                 random_state=666, n_jobs=-1)\n",
    "rf_clf2.fit(X, y)\n",
    "rf_clf2.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сверхслучайные деревья (Extra-Trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В сверхслучайных деревьях (Extremely Randomized Trees) больше случайности в том, как вычисляются разделения в узлах. Как и в случайных лесах, используется случайное подмножество возможных признаков, но вместо поиска наиболее оптимальных порогов, пороговые значения произвольно выбираются для каждого возможного признака, и наилучший из этих случайно генерируемых порогов выбирается как лучшее правило для разделения узла. Это обычно позволяет немного уменьшить дисперсию модели за счет несколько большего увеличения смещения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "et_clf = ExtraTreesClassifier(n_estimators=500, max_leaf_nodes=16, bootstrap=True, oob_score=True, \n",
    "                              random_state=666, n_jobs=-1)\n",
    "et_clf.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сверхслучайные деревья (в качестве побочного эффекта) позволяют оценить важность признаков, чтобы в дальнейшем провести отбор на основе важности признаков.\n",
    "\n",
    "В представленном ниже примере мы обучаем классификатор `ExtraTreesClassifier`, чтобы с его помощью определить важность признаков в наборе Ирисы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:,:]\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, y)\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Случайный лес в наборе данных о кредитовании\n",
    "\n",
    "Рассмотрим данные о кредитовании в файле __loan_data.csv__ и попытаемся классифицировать и предсказать, полностью ли заемщик выплатил свой кредит."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные в столбцах представляют собой следующее:\n",
    "\n",
    "* **credit.policy**: 1, если клиент соответствует критериям кредитного андеррайтинга, и 0 в противном случае.\n",
    "* **purpose**: цель ссуды (принимает значения \"credit_card\", \"debt_consolidation\", \"educational\", \"major_purchase\", \"small_business\" и \"all_other\").\n",
    "* **int.rate**: процентная ставка по ссуде в виде пропорции (ставка 11% будет сохранена как 0,11). Более рискованным заемщикам назначаются более высокие процентные ставки.\n",
    "* **installment**: ежемесячные платежи, подлежащие уплате заемщиком, если ссуда финансируется.\n",
    "* **log.annual.inc**: Натуральный логарифм годового дохода заемщика, представленный самооценкой.\n",
    "* **dti**: отношение долга к доходу заемщика (debt-to-income, сумма долга, деленная на годовой доход).\n",
    "* **fico**: кредитный рейтинг FICO заемщика.\n",
    "* **days.with.cr.line**: количество дней, в течение которых у заемщика была кредитная линия.\n",
    "* **revol.bal**: возобновляемый баланс заемщика (сумма, не выплаченная в конце платежного цикла кредитной карты).\n",
    "* **revol.util**: коэффициент использования возобновляемой линии заемщика (сумма использованной кредитной линии по отношению к общему доступному кредиту).\n",
    "* **inq.last.6mths**: количество запросов от кредиторов заемщиком за последние 6 месяцев.\n",
    "* **delinq.2yrs**: количество случаев просрочки платежа заемщиком на 30+ дней за последние 2 года.\n",
    "* **pub.rec**: количество негативных публичных записей заемщика (заявления о банкротстве, налоговые залоги или судебные решения).\n",
    "* **not.fully.paid**: количество процентов для классификации - выплатил ли заемщик деньги полностью или нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('loan_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Данные об одобрении кредитов. 1 означает одобренный кредит, 0 - не одобренный.\")\n",
    "print(df['credit.policy'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['credit.policy']==1]['fico'].plot.hist(bins=30,alpha=0.5,color='blue', label='Credit.Policy=1')\n",
    "df[df['credit.policy']==0]['fico'].plot.hist(bins=30,alpha=0.5, color='red', label='Credit.Policy=0')\n",
    "plt.legend(fontsize=15)\n",
    "plt.title (\"Гистограмма скорингового балла FICO \\nпо одобренным и не одобренным кредитам\", fontsize=15)\n",
    "plt.xlabel(\"FICO score\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Признак **purpose** является категориальным. Преобразуем его в числовые признаки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.get_dummies(df,['purpose'],drop_first=True)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_final.drop('not.fully.paid',axis=1)\n",
    "y = df_final['not.fully.paid']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(criterion='gini',max_depth=None)\n",
    "dtree.fit(X_train,y_train)\n",
    "predictions = dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plot_tree(dtree); # max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test,predictions)\n",
    "print(cm)\n",
    "print (\"Accuracy of prediction:\",round((cm[0,0]+cm[1,1])/cm.sum(),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тренируем случайный лес и выполняем предсказание:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=1000)\n",
    "rfc.fit(X_train, y_train)\n",
    "rfc_pred = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = classification_report(y_test,rfc_pred)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,rfc_pred)\n",
    "print(cm)\n",
    "print (\"Accuracy of prediction:\",round((cm[0,0]+cm[1,1])/cm.sum(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsimu = 21\n",
    "accuracy=[0]*nsimu\n",
    "ntree = [0]*nsimu\n",
    "for i in range(1,nsimu):\n",
    "    rfc = RandomForestClassifier(n_estimators=i*5,\n",
    "                                 min_samples_split=10,max_depth=None,criterion='gini')\n",
    "    rfc.fit(X_train, y_train)\n",
    "    rfc_pred = rfc.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,rfc_pred)\n",
    "    accuracy[i] = (cm[0,0]+cm[1,1])/cm.sum()\n",
    "    ntree[i]=i*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(x=ntree[1:nsimu],y=accuracy[1:nsimu],s=60,c='red')\n",
    "plt.title(\"Число деревьев в случайном лесу и точность прогноза (критерий: 'gini')\", \n",
    "          fontsize=15)\n",
    "plt.xlabel(\"Number of trees\", fontsize=14)\n",
    "plt.ylabel(\"Точность прогноза по матрице ошибок\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бустинг (boosting) - это ансамблевый метод, объединяющий нескольких слабых классификаторов, чтобы сформировать сильный классификатор. Под слабым классификатором понимается модель, которая работает немногим лучше случайного выбора. Самыми популярными методами бустинга являются:\n",
    "* AdaBoost\n",
    "* градиентный бустинг\n",
    "\n",
    "Основная идея метода бустинга состоит в том, чтобы тщательно отбирать точки, чтобы повысить качество классификации для трудно классифицируемых точек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons(n_samples=500, noise=0.3, random_state=666) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "\n",
    "* Адаптивный бустинг (__Ada__ptive __Boost__ing)\n",
    "* Каждый следующий классификатор уделяет больше внимания случаям, ошибочно предсказанным его предшественниками.\n",
    "* Достигается за счет изменения весов в обучающихся классификаторах.\n",
    "* Каждому классификатору присваивается коэффициент, который зависит от ошибки обучения классификатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=2), n_estimators=500)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "ada_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В рассматриваемом примере можно спрогнозировать вероятности получения положительного класса в тестовой выборке при помощи метода `predict_proba()`, извлекая эти вероятности из второго  столбца следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = ada_clf.predict_proba(X_test)[:,1]\n",
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно оценить классификатор `ada_clf` при помощи показателя `ROC_AUC`. Напомним, что показатель `ROC_AUC` для бинарного классификатора можно определить с помощью функции `roc_auc_score()` из `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fprs, tprs, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "plt.plot(fprs, tprs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Evaluate test-set roc_auc_score\n",
    "ada_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print roc_auc_score\n",
    "print('ROC AUC score: {:.2f}'.format(ada_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод градиентного бустинга\n",
    "\n",
    "* Последовательное исправление ошибок предшествующего предиктора.\n",
    "* Веса точек обучающей выборки не изменяются.\n",
    "* Каждый предиктор обучается с использованием остаточных ошибок своего предшественника в качестве меток.\n",
    "* Деревья с градиентным бустингом: CART используется в качестве базового алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(max_depth=2, n_estimators=30)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "gb_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = gb_clf.predict_proba(X_test)[:,1]\n",
    "fprs, tprs, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "plt.plot(fprs, tprs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate test-set roc_auc_score\n",
    "gb_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print roc_auc_score\n",
    "print('ROC AUC score: {:.2f}'.format(gb_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание на лабораторную работу №6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание (10 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для закрепленного за Вами варианта лабораторной работы:\n",
    "\n",
    "1.\tСчитайте заданный набор данных из репозитария UCI, включая указанный в индивидуальном задании столбец с метками классов. \n",
    "\n",
    "2.\tЕсли среди меток класса имеются пропущенные значения, то удалите записи с пропущенными метками класса. Преобразуйте категориальные признаки в числовые при помощи кодирования меток (label encoding). Если в признаках имеются пропущенные значения, то замените пропущенные значения, используя метод, указанный в индивидуальном задании.\n",
    "\n",
    "3.\tИспользуя метод снижения размерности данных, указанный в индивидуальном задании, определите и оставьте в наборе данных пять признаков.\n",
    "\n",
    "4.\tНормализуйте оставшиеся признаки набора данных методом, указанным в индивидуальном задании.\n",
    "\n",
    "5.\tВизуализируйте набор данных в виде точек в трехмерном пространстве, отображая точки разных классов разными цветами. При визуализации набора данных используйте три признака с наиболее высокой оценкой важности. В качестве подписей осей используйте названия признаков. В подписи рисунка укажите название набора данных. Создайте легенду набора данных.\n",
    "\n",
    "6.\tРазбейте набор данных на обучающую и тестовую выборки. Создайте и обучите классификатор на основе деревьев решений с глубиной дерева не более 5, определите долю верных ответов на тестовой выборке и визуализируйте границу принятия решений и построенное дерево решений. При визуализации границы принятия решений используйте два признака с наиболее высокой оценкой важности.\n",
    "\n",
    "7. \tПостройте и обучите дополнительные базовые классификаторы, указанные в индивидуальном задании, затем постройте из классификатора дерева решений и дополнительных классификаторов комбинированный классификатор, указанный в индивидуальном задании. Оцените производительность базовых классификаторов и комбинированного классификатора по показателю, указанному в индивидуальном задании. \n",
    "\n",
    "8. \tПостройте и обучите пару ансамблевых классификаторов, указанных в индивидуальном задании, и сравните их производительность по показателю, указанному в индивидуальном задании. \n",
    "\n",
    "9. \tПостройте границы принятия решений ансамблевых классификаторов с визуализацией точек набора данных разных классов разными цветами. Подпишите оси и рисунок. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
