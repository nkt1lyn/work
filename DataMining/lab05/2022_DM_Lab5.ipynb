{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №5\n",
    "\n",
    "## Примеры заданий итогового теста\n",
    "\n",
    "Дано разбиение набора данных с числовыми признаками на $k$ кластеров. Найти расстояние (евклидово $L_2$, манхэттенское $L_1$, Чебышева $L_{\\infty}$) между кластерами:\n",
    "\n",
    "* методом одиночной связи (single link) $$\\delta\\left(C_{i},C_{j}\\right)=\\min_{\\mathbf{\\overline{x}},\\mathbf{\\overline{y}}}\\left\\{ \\delta\\left(\\mathbf{\\overline{x}},\\mathbf{\\overline{y}}\\right)\\mid\\mathbf{\\overline{x}}\\in C_{i},\\mathbf{\\overline{y}}\\in C_{j}\\right\\}$$ \n",
    "\n",
    "* методом полной связи (complete link) $$\\delta\\left(C_{i},C_{j}\\right)=\\max_{\\mathbf{\\overline{x}},\\mathbf{\\overline{y}}}\\left\\{ \\delta\\left(\\mathbf{\\overline{x}},\\mathbf{\\overline{y}}\\right)\\mid\\mathbf{\\overline{x}}\\in C_{i},\\mathbf{\\overline{y}}\\in C_{j}\\right\\}$$ \n",
    "\n",
    "* методом средней связи (group average) \n",
    "$$\\delta\\left(C_{i},C_{j}\\right)=\\frac{1}{n_{i}n_{j}}\\sum_{\\mathbf{\\overline{x}}\\in C_{i}}\\sum_{\\mathbf{\\overline{y}}\\in C_{j}}\\delta\\left(\\mathbf{\\overline{x}},\\mathbf{\\overline{y}}\\right)$$\n",
    "\n",
    "* методом Уарда (Ward’s measure) \n",
    "$$\\delta\\left(C_{i},C_{j}\\right)=\\frac{n_{i}n_{j}}{n_{i}+n_{j}}\\left\\Vert \\overline{\\mu}_{i}-\\overline{\\mu}_{j}\\right\\Vert ^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ Дано разбиение набора данных с двумя числовыми признаками на два кластера:\n",
    "\n",
    "* кластер 1 ($C_{1}$): $\\left\\{ \\left(0,0\\right),\\,\\left(0,1\\right),\\,\\left(1,0\\right)\\right\\}$ \n",
    "\n",
    "* кластер 2 ($C_{2}$): $\\left\\{ \\left(2,2\\right),\\,\\left(3,3\\right)\\right\\}$ \n",
    "\n",
    "Найти евклидово расстояние между кластерами методом одиночной связи (single link).\n",
    "\n",
    "__Решение:__ Расстояние между кластерами методом одиночной связи (single link) вычисляется по формуле\n",
    "\n",
    "$$\\delta\\left(C_{i},C_{j}\\right)=\\min_{\\mathbf{\\overline{x}},\\mathbf{\\overline{y}}}\\left\\{ \\delta\\left(\\mathbf{\\overline{x}},\\mathbf{\\overline{y}}\\right)\\mid\\mathbf{\\overline{x}}\\in C_{i},\\mathbf{\\overline{y}}\\in C_{j}\\right\\}$$ \n",
    "\n",
    "Евклидово расстояние вычисляется по формуле \n",
    "\n",
    "$$\\rho\\left(\\mathbf{\\overline{x}},\\mathbf{\\overline{y}}\\right)=\\sqrt{\\sum_{i=1}^{d}\\left(x_{i}-y_{i}\\right)^{2}},\\,\\mathbf{\\overline{x}}=\\left(x_{1},...,x_{d}\\right),\\mathbf{\\overline{y}}=\\left(y_{1},...,y_{d}\\right)$$\n",
    "\n",
    "Наиболее близкими точками двух кластеров являются точка $\\left(0,\\,1\\right)$ (или точка $\\left(1,\\,0\\right)$) и точка $\\left(2,\\,2\\right)$, поэтому \n",
    "\n",
    "$$\\delta\\left(C_{1},C_{2}\\right)=\\rho\\left(\\left(0,\\,1\\right),\\,\\left(2,\\,2\\right)\\right)=\\sqrt{2^{2}+1^{2}}=\\sqrt{5}=2.236$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дана таблица (матрица) сопряженности кластеризации. Вычислить:\n",
    "\n",
    "* чистоту (purity) или точность (precision) кластеров и чистоту кластеризации\n",
    "$$p_{j}=prec_{j}=\\frac{1}{m_{j}}\\max_{i=\\overline{1,k}}n_{ij}=\\frac{n_{i_{j}j}}{m_{j}},\\,i_{j}=\\max_{i=\\overline{1,k}}n_{ij},\\,j=\\overline{1,r}$$\n",
    "$$p=\\sum_{j=1}^{r}\\frac{m_{j}}{n}\\,p_{j}=\\frac{1}{n}\\,\\sum_{j=1}^{r}\\max_{i=\\overline{1,k}}n_{ij},$$\n",
    "\n",
    "* полноту (recall) кластеров \n",
    "$$recall_{j}=\\frac{n_{i_{j}j}}{\\left|T_{i_{j}}\\right|}=\\frac{n_{i_{j}j}}{n_{i_{j}}},\\,n_{i_{j}}=\\left|T_{i_{j}}\\right|,\\,j=\\overline{1,r},$$\n",
    "\n",
    "* F-меру кластеров и F-меру кластеризации\n",
    "$$F_{j}=\\frac{2}{\\frac{1}{prec_{j}}+\\frac{1}{recall_{j}}}=\\frac{2n_{i_{j}j}}{n_{i_{j}}+m_{j}},\\,j=\\overline{1,r},\\,F=\\frac{1}{r}\\sum_{j=1}^{r}F_{j}$$\n",
    "\n",
    "* энтропию кластеризации (разбиения на кластеры)\n",
    "$$H\\left(\\mathcal{C}\\right)=-\\sum_{j=1}^{r}p_{C_{j}}\\log_{2}p_{C_{j}},\\,p_{C_{j}}=\\frac{m_{j}}{n}$$\n",
    "\n",
    "* энтропию разбиения на классы \n",
    "$$H\\left(\\mathcal{T}\\right)=-\\sum_{i=1}^{k}p_{T_{i}}\\log_{2}p_{T_{i}},\\,p_{T_{i}}=\\frac{n_{i}}{n}$$\n",
    "\n",
    "* условную энтропию относительно кластера\n",
    "$$H\\left(\\mathcal{T}\\mid C_{j}\\right)=-\\sum_{i=1}^{k}\\left(\\frac{n_{ij}}{n_{i}}\\right)\\log_{2}\\left(\\frac{n_{ij}}{n_{i}}\\right)$$\n",
    "\n",
    "* условную энтропию относительно кластеризации\n",
    "$$H\\left(\\mathcal{T}\\mid\\mathcal{C}\\right)=\\sum_{j=1}^{r}\\frac{m_{j}}{n}H\\left(\\mathcal{T}\\mid C_{j}\\right)=-\\sum_{j=1}^{r}\\sum_{i=1}^{k}p_{ij}\\log_{2}\\left(\\frac{p_{ij}}{p_{C_{j}}}\\right),\\,p_{ij}=\\frac{n_{ij}}{n}$$\n",
    "\n",
    "* парные меры FN, FP, TP, TN\n",
    "$$TP=\\left|\\left\\{ \\left(\\mathbf{x}_{i},\\mathbf{x}_{j}\\right):y_{i}=y_{j},\\hat{y}_{i}=\\hat{y}_{j}\\right\\} \\right|=\\sum_{i=1}^{k}\\sum_{j=1}^{r}\\left(\\begin{array}{c}\n",
    "n_{ij}\\\\\n",
    "2\n",
    "\\end{array}\\right)=\\frac{1}{2}\\left(\\left(\\sum_{i=1}^{k}\\sum_{j=1}^{r}n_{ij}^{2}\\right)-n\\right)$$\n",
    "\n",
    "$$FN=\\left|\\left\\{ \\left(\\mathbf{x}_{i},\\mathbf{x}_{j}\\right):y_{i}=y_{j},\\hat{y}_{i}\\neq\\hat{y}_{j}\\right\\} \\right|=\\sum_{i=1}^{k}\\left(\\begin{array}{c}\n",
    "n_{i}\\\\\n",
    "2\n",
    "\\end{array}\\right)-TP=\\frac{1}{2}\\left(\\sum_{i=1}^{k}n_{i}^{2}-\\sum_{i=1}^{k}\\sum_{j=1}^{r}n_{ij}^{2}\\right)$$\n",
    "\n",
    "$$FP=\\left|\\left\\{ \\left(\\mathbf{x}_{i},\\mathbf{x}_{j}\\right):y_{i}\\neq y_{j},\\hat{y}_{i}=\\hat{y}_{j}\\right\\} \\right|=\\sum_{j=1}^{r}\\left(\\begin{array}{c}\n",
    "m_{j}\\\\\n",
    "2\n",
    "\\end{array}\\right)-TP=\\frac{1}{2}\\left(\\sum_{j=1}^{r}m_{j}^{2}-\\sum_{i=1}^{k}\\sum_{j=1}^{r}n_{ij}^{2}\\right)$$\n",
    "\n",
    "$$TN=\\left|\\left\\{ \\left(\\mathbf{x}_{i},\\mathbf{x}_{j}\\right):y_{i}\\neq y_{j},\\hat{y}_{i}\\neq\\hat{y}_{j}\\right\\} \\right|=N-\\left(TP+FN+FP\\right)==\\frac{1}{2}\\left(n^{2}-\\sum_{i=1}^{k}n_{i}^{2}-\\sum_{j=1}^{r}m_{j}^{2}+\\sum_{i=1}^{k}\\sum_{j=1}^{r}n_{ij}^{2}\\right)$$\n",
    "\n",
    "* индекс Rand\n",
    "$$Rand=\\frac{TP+TN}{TP+TN+FP+FN}$$\n",
    "\n",
    "* индекс Жаккара (Jaccard Index) \n",
    "$$Jaccard=\\frac{TP}{TP+FP+FN}$$\n",
    "\n",
    "* индекс Фоулкса – Мэллоуса (Fowlkes-Mallows Index) \n",
    "$$FM=\\sqrt{\\frac{TP}{TP+TN}\\frac{TP}{TP+FP}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ Дана таблица сопряженности кластеризации с двумя классами и тремя кластерами:\n",
    "\n",
    "$$\\left(\\begin{array}{ccc}\n",
    "5 & 5 & 5\\\\\n",
    "3 & 10 & K\n",
    "\\end{array}\\right),$$\n",
    "\n",
    "где $K$ – параметр, принимающий целое положительное значение, причем $K>5$. \n",
    "\n",
    "Вычислить чистоту (purity) каждого кластера.\n",
    "\n",
    "__Решение:__ Чистота (purity) кластеров задается формулами\n",
    "\n",
    "$$p_{j}=\\frac{1}{m_{j}}\\max_{i=\\overline{1,k}}n_{ij}=\\frac{n_{i_{j}j}}{m_{j}},\\,i_{j}=\\arg\\max_{i=\\overline{1,k}}n_{ij},\\,j=\\overline{1,r}$$\n",
    "\n",
    "Строим расширенную таблицу сопряженности:\n",
    "\n",
    "$$\\left(\\begin{array}{ccc|c}\n",
    "5 & 5 & 5 & 15\\\\\n",
    "3 & 10 & K & K+13\\\\\n",
    "\\hline 8 & 15 & K+5 & K+28\n",
    "\\end{array}\\right)$$\n",
    "\n",
    "и определяем чистоту кластеров:\n",
    "\n",
    "$$p_{1}=\\frac{5}{8}=0.625,\\,p_{2}=\\frac{10}{15}=0.667,\\,p_{3}=\\frac{K}{K+5}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Бинарная классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бинарная (двоичная) классификация (binary classification) — это задача классификации элементов заданного набора данных в два класса. \n",
    "\n",
    "Для бинарной классификации могут применяться методы многоклассовой классификации, а также более специализированные методы, такие, как метод опорных векторов, логистическая регрессия, линейный дискриминантный анализ и др."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия — это статистическая модель для бинарной классификации, использующая логистическую функцию (кривую). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая функция (сигмоида)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return 1. / (1. + np.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 500)\n",
    "\n",
    "plt.plot(x, sigmoid(x));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бинарная классификация при помощи логистической регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать набор данных Ирисы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставим в наборе данных первые два признака из четырех и первые два класса из трех:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[y<2,:2]\n",
    "y = y[y<2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем классы на плоскости:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y==0,0], X[y==0,1], color=\"red\")\n",
    "plt.scatter(X[y==1,0], X[y==1,1], color=\"blue\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем набор данных на обучающую и тестовую выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим и обучим классификатор логистической регрессии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификатор не допускает ошибок на тестовой выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Граница решения "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задаче бинарной классификации граница решения (поверхность решения) - это гиперповерхность, которая разделяет пространство признаков на два набора, по одному для каждого класса. Классификатор классифицирует все точки по одной стороне границы принятия решения как принадлежащие одному классу, а все точки на другой стороне как принадлежащие другому классу. На самой границе решения прогнозируемая классификатором метка класса неоднозначна. \n",
    "\n",
    "Если граница решения является гиперплоскостью, то задача классификации линейна и классы линейно разделимы. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия - это линейный классификатор и граница решения характеризуется следующими коэффициентами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, прямая, разделяющая точки набора данных задается следующим образом: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x2(x1):\n",
    "    return (-log_reg.coef_[0][0] * x1 - log_reg.intercept_[0]) / log_reg.coef_[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем эту прямую и точки исходного набора данных и тестовой выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_plot = np.linspace(4, 7, 1000)\n",
    "x2_plot = x2(x1_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y==0,0], X[y==0,1], color=\"red\")\n",
    "plt.scatter(X[y==1,0], X[y==1,1], color=\"blue\")\n",
    "plt.plot(x1_plot, x2_plot);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test[y_test==0,0], X_test[y_test==0,1], color=\"red\")\n",
    "plt.scatter(X_test[y_test==1,0], X_test[y_test==1,1], color=\"blue\")\n",
    "plt.plot(x1_plot, x2_plot);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать следующую функцию для визуализации границы решения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, axis):\n",
    "    \n",
    "    x0, x1 = np.meshgrid(\n",
    "        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),\n",
    "        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),\n",
    "    )\n",
    "    X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "    y_predict = model.predict(X_new)\n",
    "    zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])\n",
    "    \n",
    "    plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Граница решения для классификатора логистической регрессии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(log_reg, axis=[4, 7.5, 1.5, 4.5])\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Граница решения для нелинейного классификатора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим классификатор на основе метода К ближайших соседей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Граница решения теперь является нелинейной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(knn_clf, axis=[4, 7.5, 1.5, 4.5])\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим теперь классификатор на полном наборе данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf_all = KNeighborsClassifier()\n",
    "knn_clf_all.fit(iris.data[:,:2], iris.target);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Граница решения стала более извилистой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(knn_clf_all, axis=[4, 8, 1.5, 4.5])\n",
    "plt.scatter(iris.data[iris.target==0,0], iris.data[iris.target==0,1])\n",
    "plt.scatter(iris.data[iris.target==1,0], iris.data[iris.target==1,1])\n",
    "plt.scatter(iris.data[iris.target==2,0], iris.data[iris.target==2,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если метка класса определяется по 50 ближайшим соседям, то получим такую визуализацию границы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf_all = KNeighborsClassifier(n_neighbors=50)\n",
    "knn_clf_all.fit(iris.data[:,:2], iris.target)\n",
    "\n",
    "plot_decision_boundary(knn_clf_all, axis=[4, 8, 1.5, 4.5])\n",
    "plt.scatter(iris.data[iris.target==0,0], iris.data[iris.target==0,1])\n",
    "plt.scatter(iris.data[iris.target==1,0], iris.data[iris.target==1,1])\n",
    "plt.scatter(iris.data[iris.target==2,0], iris.data[iris.target==2,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод рекурсивного исключения признаков (RFE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод рекурсивного исключения признаков (recursive feature elimination, RFE) реализует следующий алгоритм: модель обучается на исходном наборе признаков и оценивает их значимость, затем исключается один или несколько наименее значимых признаков, модель обучается на оставшихся признаках, и так далее, пока не останется заданное количество лучших признаков. \n",
    "\n",
    "Метод RFE может применяться в сочетании с логистической регрессией для отбора лучших признаков: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction with RFE\n",
    "from sklearn.feature_selection import RFE\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "# feature extraction\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "rfe = RFE(model)\n",
    "fit = rfe.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Число признаков: %d\" % fit.n_features_)\n",
    "print(\"Выбранные признаки: %s\" % fit.support_)\n",
    "print(\"Ранг признаков: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбранным (оцененым как лучшие) признакам присвоен ранг 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейный дискриминантный анализ (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод анализа основных компонентов PCA определяет комбинации признаков (основные компоненты), для которых набор данных имеет наибольшую дисперсию.\n",
    "\n",
    "Метод линейного дискриминантного анализа (LDA) определяет комбинации признаков, для которых классы разделяются наилучшим образом. LDA, в отличие от PCA, является методом машинного обучения с учителем, использующим заданные  метки классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "target_names = iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit(X).transform(X)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_lda = lda.fit(X, y).transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализации для методов PCA и LDA отличаются:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "colors = ['b', 'r', 'g']\n",
    "\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], color=color, alpha=.8, lw=2,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('Метод PCA для набора Ирисы');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(X_lda[y == i, 0], X_lda[y == i, 1], alpha=.8, color=color,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('Метод LDA для набора данных Ирисы');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В методе PCA определяются главные компоненты (комбинации признаков):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В методе LDA определяется направление (вектор), наилучшим образом разделяющий точки классов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим метод LDA для классификации точек набора Ирисы (с двумя признаками и двумя классами):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Граница решения является линейной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(lda, axis=[4, 7.5, 1.5, 4.5])\n",
    "plt.scatter(X_test[y_test==0,0], X_test[y_test==0,1])\n",
    "plt.scatter(X_test[y_test==1,0], X_test[y_test==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод опорных векторов (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод опорных векторов (метод построения оптимальной разделяющей гиперплоскости) был разработан В. Вапником и А. Червоненкисом. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X = X[y<2,:2]\n",
    "y = y[y<2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y==0,0], X[y==0,1], color='red')\n",
    "plt.scatter(X[y==1,0], X[y==1,1], color='blue');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стандартизуем набор данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standardScaler = StandardScaler()\n",
    "standardScaler.fit(X)\n",
    "X_standard = standardScaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим и обучим классификатор на основе SVM с параметром регуляризации C=1e9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc = LinearSVC(C=1e9)\n",
    "svc.fit(X_standard, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(svc, axis=[-3, 3, -3, 3])\n",
    "plt.scatter(X_standard[y==0,0], X_standard[y==0,1])\n",
    "plt.scatter(X_standard[y==1,0], X_standard[y==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим и обучим на тех же данных классификатор на основе SVM с другим параметром регуляризации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc2 = LinearSVC(C=10.)\n",
    "svc2.fit(X_standard, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(svc2, axis=[-3, 3, -3, 3])\n",
    "plt.scatter(X_standard[y==0,0], X_standard[y==0,1])\n",
    "plt.scatter(X_standard[y==1,0], X_standard[y==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Граница решения определяется такими параметрами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc2.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc2.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для визуализации зазора метода SVM будем использовать следующую функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_boundary(model, axis):\n",
    "    \n",
    "    x0, x1 = np.meshgrid(\n",
    "        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),\n",
    "        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),\n",
    "    )\n",
    "    X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "    y_predict = model.predict(X_new)\n",
    "    zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])\n",
    "    \n",
    "    plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
    "    \n",
    "    w = model.coef_[0]\n",
    "    b = model.intercept_[0]\n",
    "    \n",
    "    # w0*x0 + w1*x1 + b = 0\n",
    "    # => x1 = -w0/w1 * x0 - b/w1\n",
    "    plot_x = np.linspace(axis[0], axis[1], 200)\n",
    "    up_y = -w[0]/w[1] * plot_x - b/w[1] + 1/w[1]\n",
    "    down_y = -w[0]/w[1] * plot_x - b/w[1] - 1/w[1]\n",
    "    \n",
    "    up_index = (up_y >= axis[2]) & (up_y <= axis[3])\n",
    "    down_index = (down_y >= axis[2]) & (down_y <= axis[3])\n",
    "    plt.plot(plot_x[up_index], up_y[up_index], color='black')\n",
    "    plt.plot(plot_x[down_index], down_y[down_index], color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализации для первого и второго классификаторов имеют следующий вид:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svc_decision_boundary(svc, axis=[-3, 3, -3, 3])\n",
    "plt.scatter(X_standard[y==0,0], X_standard[y==0,1])\n",
    "plt.scatter(X_standard[y==1,0], X_standard[y==1,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svc_decision_boundary(svc2, axis=[-3, 3, -3, 3])\n",
    "plt.scatter(X_standard[y==0,0], X_standard[y==0,1])\n",
    "plt.scatter(X_standard[y==1,0], X_standard[y==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация нелинейного набора данных при помощи логистической регрессии\n",
    "\n",
    "Рассмотрим следующий синтетический набор данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(666)\n",
    "X = np.random.normal(0, 1, size=(200, 2))\n",
    "y = np.array((X[:,0]**2+X[:,1]**2)<1.5, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификация набора при помощи логистической регрессии дает низкое качество классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(log_reg, axis=[-4, 4, -4, 4])\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Полиномиальные зависимости\n",
    "\n",
    "Класс `PolynomialFeatures` позволяет строить наборы данных, содержащий полиномиальные зависимости от исходных данных (столбцы $x_1, x_2$ в наборе $D$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.arange(1, 11).reshape(-1, 2)\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "polyD = PolynomialFeatures(degree=2) # степень полинома\n",
    "polyD.fit(D)\n",
    "D_2 = polyD.transform(D)\n",
    "D_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем новый набор, в котором первый столбец соответствует свободному члену (не зависит от $x_1$, $x_2$), второй и третий столбцы равны $x_1$ и $x_2$, четвертый столбец равен $x_1^2$, пятый столбец равен $x_1 x_2$, шестой столбец равен $x_2^2$. \n",
    "\n",
    "Таким образом, от признаков $(x_1, x_2)$ переходим к расширенному набору признаков $(1, x_1, x_2, x_1^2, x_1 x_2, x_2^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построение конвейера (Pipeline)\n",
    "\n",
    "Цель конвейера (pipeline) состоит в том, чтобы собрать несколько преобразований и заключительный классификатор, которые могут быть исполнены вместе с установкой различных параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def PolynomialLogisticRegression(degree):\n",
    "    return Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "        ('log_reg', LogisticRegression())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим конвейер с параметром `degree=2` для классификации нелинейного набора, приведенного выше:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_log_reg = PolynomialLogisticRegression(degree=2)\n",
    "poly_log_reg.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_log_reg.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(poly_log_reg, axis=[-4, 4, -4, 4])\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно увеличить параметр `degree` до 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_log_reg2 = PolynomialLogisticRegression(degree=20)\n",
    "poly_log_reg2.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_log_reg2.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(poly_log_reg2, axis=[-4, 4, -4, 4])\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Параметры логистической регрессии\n",
    "\n",
    "Реализация логистической регрессии в __sklearn__ допускает некоторое количество параметров, в частности:\n",
    "\n",
    "* `penalty` - штраф, принимающий значения ‘l1’, ‘l2’, ‘elasticnet’ или ‘none’\n",
    "* `C` - константа регуляризации (обратно пропорциональна силе регуляризации)\n",
    "* `solver` - алгоритм, используемый для оптимизации, принимающий значения ‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’\n",
    "\n",
    "Рассмотрим другой синтетический набор данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.normal(0, 1, size=(200, 2))\n",
    "y = np.array((X[:,0]**2+X[:,1])<1.5, dtype='int')\n",
    "for _ in range(20):\n",
    "    y[np.random.randint(200)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.score(X_train, y_train), log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(log_reg, axis=[-4, 4, -4, 4])\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialLogisticRegression(degree=2, C=1.):\n",
    "    return Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "        ('log_reg', LogisticRegression(C=C))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_log_reg3 = PolynomialLogisticRegression(degree=2, C=0.1)\n",
    "poly_log_reg3.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_log_reg3.score(X_train, y_train), poly_log_reg3.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификация нелинейного набора данных при помощи SVM\n",
    "\n",
    "Создадим нелинейный набор данных в форму пулумесяцев:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "X, y = datasets.make_moons()\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим шум:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons(noise=0.15, random_state=666)\n",
    "\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим конвейер на основе SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialSVC(degree, C=1.0):\n",
    "    return Pipeline([\n",
    "        (\"poly\", PolynomialFeatures(degree=degree)),\n",
    "        (\"std_scaler\", StandardScaler()),\n",
    "        (\"linearSVC\", LinearSVC(C=C))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_svc = PolynomialSVC(degree=4, C=100.)\n",
    "poly_svc.fit(X, y)\n",
    "poly_svc.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(poly_svc, axis=[-1.5, 2.5, -1.0, 1.5])\n",
    "plt.scatter(X[y==0,0], X[y==0,1])\n",
    "plt.scatter(X[y==1,0], X[y==1,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Матрица ошибок, точность и полнота"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продемонстрируем меры бинарной классификации на примере набора данных с изображениеми цифр:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target.copy()\n",
    "\n",
    "y[digits.target==9] = 1\n",
    "y[digits.target!=9] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим классификатор логистической регрессии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_log_predict = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Показатели TN, FP, FN и TP могут быть реализованы так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TN(y_true, y_predict):\n",
    "    assert len(y_true) == len(y_predict)\n",
    "    return np.sum((y_true == 0) & (y_predict == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN(y_test, y_log_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FP(y_true, y_predict):\n",
    "    assert len(y_true) == len(y_predict)\n",
    "    return np.sum((y_true == 0) & (y_predict == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP(y_test, y_log_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FN(y_true, y_predict):\n",
    "    assert len(y_true) == len(y_predict)\n",
    "    return np.sum((y_true == 1) & (y_predict == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN(y_test, y_log_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TP(y_true, y_predict):\n",
    "    assert len(y_true) == len(y_predict)\n",
    "    return np.sum((y_true == 1) & (y_predict == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP(y_test, y_log_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матрица ошибок для бинарной классификации определяется так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_true, y_predict):\n",
    "    return np.array([\n",
    "        [TN(y_true, y_predict), FP(y_true, y_predict)],\n",
    "        [FN(y_true, y_predict), TP(y_true, y_predict)]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_log_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Показатель точности для положительного класса может быть вычислен так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_score(y_true, y_predict):\n",
    "    tp = TP(y_true, y_predict)\n",
    "    fp = FP(y_true, y_predict)\n",
    "    try:\n",
    "        return tp / (tp + fp)\n",
    "    except:\n",
    "        return 0.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_log_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Показатель TPR (доля корректно спрогнозированных положительных точек) вычисляется так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_score(y_true, y_predict):\n",
    "    tp = TP(y_true, y_predict)\n",
    "    fn = FN(y_true, y_predict)\n",
    "    try:\n",
    "        return tp / (tp + fn)\n",
    "    except:\n",
    "        return 0.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_log_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При использовании функций из scikit-learn получим аналогичные результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_log_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision_score(y_test, y_log_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "recall_score(y_test, y_log_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кривая полнота-точность (precision-recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кривая полнота-точность строится в координатах полнота (recall) и точность (precision). Площадь под кривой часто используют в качестве метрики качества алгоритма. Площадь под кривой полнота-точность  рекомендуют использовать в задачах с дисбалансом классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target.copy()\n",
    "\n",
    "y[digits.target==9] = 1\n",
    "y[digits.target!=9] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "decision_scores = log_reg.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "thresholds = np.arange(np.min(decision_scores), np.max(decision_scores), 0.1)\n",
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for threshold in thresholds:\n",
    "    y_predict = np.array(decision_scores >= threshold, dtype='int')\n",
    "    precisions.append(precision_score(y_test, y_predict))\n",
    "    recalls.append(recall_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thresholds, precisions, c='r')\n",
    "plt.plot(thresholds, recalls, c='b');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(precisions, recalls);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, decision_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thresholds, precisions[:-1])\n",
    "plt.plot(thresholds, recalls[:-1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recalls, precisions);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ ROC кривых"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Площадь под ROC-кривой – одна из самых популярных мер качества в задачах бинарной классификации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target.copy()\n",
    "\n",
    "y[digits.target==9] = 1\n",
    "y[digits.target!=9] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для ROC-анализа нужны значения т.н. скоринговой функции для каждой точки в тестовом наборе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "decision_scores = log_reg.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вдоль осей откладываются показатели FPR (ось абсцисс) и TPR (ось ординат):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fprs = []\n",
    "tprs = []\n",
    "n = len(X_test)\n",
    "thresholds = np.arange(np.min(decision_scores), np.max(decision_scores), 0.1)\n",
    "for threshold in thresholds:\n",
    "    y_predict = np.array(decision_scores >= threshold, dtype='int')\n",
    "    fprs.append(FP(y_test, y_predict)/n)\n",
    "    tprs.append(TP(y_test, y_predict)/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fprs, tprs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование библиотечной функции дает тот же результат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fprs, tprs, thresholds = roc_curve(y_test, decision_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fprs, tprs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для вычисления площади под кривой можно использовать метод трапеций или библиотечную функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_test, decision_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание на лабораторную работу №5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание (10 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для закрепленного за Вами варианта лабораторной работы:\n",
    "\n",
    "1.\tСчитайте заданный набор данных из репозитария UCI, включая указанный в индивидуальном задании столбец с метками классов.  \n",
    "\n",
    "2.\tЕсли среди меток класса имеются пропущенные значения, то удалите записи с пропущенными метками класса. Если столбец с метками классов содержит более двух классов, то объедините некоторые классы, чтобы получить набор для бинарной классификации с примерно равным количеством точек в положительном и отрицательном классах. Если один из классов является преобладающим (мажоритарным), то объедините все прочие классы в другой класс. \n",
    "\n",
    "3.\tЕсли какие-либо числовые признаки в наборе были распознаны неверно, то преобразуйте их в числовые. Удалите из набора признаки с текстовыми (категориальными) значениями. Если в оставшихся числовых признаках имеются пропущенные значения, то замените их на средние значения для положительного и отрицательного классов.  \n",
    "\n",
    "4.\tВыполните стандартизацию признаков набора данных.  \n",
    "\n",
    "5.\tИспользуя метод отбора признаков, указанный в индивидуальном задании, определите и оставьте в наборе данных два наиболее значимых признака, принимающих более 10 различных значений.\n",
    "\n",
    "6.\tВизуализируйте набора данных в виде точек на плоскости, отображая точки положительного и отрицательного классов разными цветами и разными маркерами. В качестве подписей осей используйте названия признаков, согласно описания набора данных. В подписи рисунка укажите название набора данных. Создайте легенду набора данных.\n",
    "\n",
    "7.\tСоздайте модели классификации точек набора данных из двух признаков на базе классификаторов, указанных в индивидуальном задании. Используйте при обучении классификаторов разделение набора данных на обучающую и тестовую выборки в соотношении 70% на 30%. \n",
    "\n",
    "8. \tВизуализируйте для каждого из классификаторов границу принятия решения, подписывая оси и рисунок и создавая легенду для меток классов набора данных в соответствии с требованиями п. 6.  \n",
    "\n",
    "9. \tВизуализируйте на одном рисунке кривые бинарной классификации, указанные в индивидуальном задании, для каждого из классификаторов, подписывая оси и рисунок. Используйте в качестве меток легенды для названия классификаторов.  \n",
    "\n",
    "10.\tОпределите лучший из используемых методов бинарной классификации по показателю площади, ограниченной кривой из п. 9.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
