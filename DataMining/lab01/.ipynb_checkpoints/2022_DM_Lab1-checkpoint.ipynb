{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №1\n",
    "# Препроцессинг данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Препроцессинг данных включает в себя широкий круг методов для очистки, выбора и преобразования данных с целью улучшения качества последующего интеллектуального анализа данных. \n",
    "\n",
    "Программные средства для препроцессинга данных имеются как в библиотеке Pandas, так и основной библиотеке машинного обучения scikit-learn (sklearn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных из удаленного файла\n",
    "\n",
    "Считаем набор данных “Ирисы” из репозитария UCI (http://archive.ics.uci.edu/) различными способами.\n",
    "\n",
    "1) Считаем данные при помощи библиотеки `urllib.request`, выведем данные на экран и проанализируем размерность данных (количество записей и признаков):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# данные из репозитария UCI\n",
    "url = \\\n",
    "    \"https://archive.ics.uci.edu/ml/\"+\\\n",
    "    \"machine-learning-databases/iris/iris.data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "data = urllib.request.urlopen(url) # объект типа 'HTTPResponse'\n",
    "\n",
    "xList = []\n",
    "for line in data:    \n",
    "    row = line.strip().decode().split(\",\") # сплит по запятой\n",
    "    if len(row) > 1:\n",
    "        xList.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('##### Набор данных Ирисы #####')\n",
    "print(\"Число строк = \", len(xList))\n",
    "print(\"Число столбцов = \", len(xList[1]))\n",
    "xList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы использовать считанные данные, нужно преобразовать их в правильный тип.\n",
    "\n",
    "2) Скопируем файл из репозитария UCI на локальный диск, считаем набор данных при помощи функции `genfromtxt()` библиотеки NumPy и дополнительно рассчитаем средние значения признаков, матрицы ковариаций и корреляций признаков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from contextlib import closing\n",
    "\n",
    "# копируем удаленный файл на диск\n",
    "with closing(urlopen(url)) as u, open(\"iris.csv\", \"w\") as f: \n",
    "    f.write(u.read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.genfromtxt( \"iris.csv\", delimiter=\",\", usecols=(0,1,2,3), \n",
    "                     dtype=float ) \n",
    "targ = np.genfromtxt( \"iris.csv\", delimiter=\",\", usecols=(4), \n",
    "                     dtype=str )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_mean = np.mean( data, axis=0 )\n",
    "iris_cov = np.cov( data.T )\n",
    "iris_corr = np.corrcoef( data.T ) \n",
    "\n",
    "print( \"*** Средние значения:\\n\", iris_mean )\n",
    "print( \"*** Матрица ковариаций:\\n\", iris_cov )\n",
    "print( \"*** Матрица корреляций:\\n\", iris_corr )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Считаем теперь набор данных “Ирисы” при помощи пакета Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# считываем данные в объект DataFrame\n",
    "my_data = pd.read_csv( url, header=None )\n",
    "my_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.describe() # сводка данных для числовых столбцов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Плохое качество данных оказывает негативное воздействие на процесс анализа данных. Наиболее часто встречающиеся проблемы включают шум, выбросы, отсутствующие значения и дублирующиеся данные.\n",
    "\n",
    "## Работа с пропущенными значениями\n",
    "\n",
    "Достаточно часто в данных отсутствуют (пропущено) одно или несколько значений признаков. Иногда не хватает информации, а иногда некоторые значения не подходят для признаков. Существуют различные подходы для работы с пропущенными (отсутствующими) значениями. \n",
    "\n",
    "Рассмотрим инструментарий для работы с пропущенными значениями на примере синтетического набора данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.arange(0, 15).reshape(5, 3), \n",
    "                  index=['r1', 'r2', 'r3', 'r4', 'r5'], \n",
    "                  columns=['c1', 'c2', 'c3'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значение `nan` (not-a-number) представляет собой специальное значение, определенное в библиотеке NumPy и предназначенное для кодирования пустых значений: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nan, type(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем в датафрейме ряд изменений (использован индексатор `loc`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['c4'] = np.nan               # новый столбец со значениями NaN\n",
    "df.loc['r6'] = np.arange(15,19) # новая строка со значениями от 15 до 18\n",
    "df.loc['r7'] = np.nan           # новая строка со значениями NaN\n",
    "df['c5'] = np.nan               # новый столбец со значениями NaN\n",
    "df['c4']['r1'] = 20             # значение NaN заменяем на 20\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения `NaN` интерпретируются как неопределенные (пропущенные).\n",
    "\n",
    "### Поиск (отбор) пропущенных значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull() # отбор элементов со значениями NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.notnull() # отбор элементов со значениями, отличными от NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "~df.isnull() # можно отобрать элементы и так"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum(axis=0) # подсчитываем кол-во NaN в каждом столбце"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum(axis=1) # подсчитываем кол-во NaN в каждой строке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count(axis=0) # кол-во значений, отличных от NaN, по каждому столбцу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаление пропущенных значений\n",
    "\n",
    "Отберем непропущенные значения в столбце `c4`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.c4[df.c4.notnull()] # один вариант обращения к столбцу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['c4'][df['c4'].notnull()] # другой вариант обращения к столбцу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно удалить из столбца все значения NaN при помощи метода `dropna()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['c4'].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `dropna()` возвращает копию с удаленными значениями, при этом  исходный датафрейм (столбец) не изменяется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `dropna()` при применении к датафрейму удаляет целиком строки, в которых есть по крайней мере одно значение NaN, поэтому из датафрейма будут удалены все строки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При использовании ключа `how='all'` удаляются лишь те строки, в которых все значения являются значениями NaN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(how = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно изменить ось, чтобы удалить столбцы со значениями NaN вместо строк:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(how='all', axis=1) # удаляем столбец c5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим копию датафрейма и заменим в двух ячейках значения NaN на  значения 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "df2.loc['r7','c1'] = 0 # df2.loc['r7'].c1=0 или df2.loc['r7']['c1']=0\n",
    "df2.loc['r7','c3'] = 0 # df2.loc['r7'].c3=0 или df2.loc['r7']['c3']=0 \n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим столбцы, в которых есть хотя бы одно значение NaN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dropna(how='any', axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставим столбцы, в которых есть по крайней мере два значения, отличных от NaN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dropna(thresh=3, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заполнение пропущенных значений\n",
    "\n",
    "Пропущенные значения могут быть заполнены константой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения NaN не учитываются при вычислении средних значений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому после замены значений NaN на 0 получаем другие средние значения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пропущенные значения могут быть заполнены соседними значениями в прямом и обратном порядке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.c4.fillna(method=\"ffill\") # прямой порядок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.c4.fillna(method=\"bfill\") # обратный порядок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заполнить пропущенные значения для конкретных индексов строк при помощи соответствующего объекта `Series`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_values = pd.Series([100, 101, 102], index=['r1', 'r2', 'r3'])\n",
    "fill_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.c4.fillna(fill_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполним значения NaN в каждом столбце средним значением этого столбца (где оно может быть вычислено):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим теперь работу с пропущенными значениями на примере набора данных из репозитария UCI с информацией о пациентах с раком груди. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/'+\\\n",
    "      'breast-cancer-wisconsin/breast-cancer-wisconsin.data'\n",
    "\n",
    "data = pd.read_csv(url, header=None)\n",
    "data.columns = ['Sample code', 'Clump Thickness', \n",
    "                'Uniformity of Cell Size', 'Uniformity of Cell Shape',\n",
    "                'Marginal Adhesion', 'Single Epithelial Cell Size', \n",
    "                'Bare Nuclei', 'Bland Chromatin',\n",
    "                'Normal Nucleoli', 'Mitoses','Class']\n",
    "\n",
    "data = data.drop(['Sample code'],axis=1) # удаляем ненужный столбец\n",
    "print('Число записей = %d' % (data.shape[0]))\n",
    "print('Число признаков = %d' % (data.shape[1]))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В наборах данных репозитария UCI пропущенные значения часто кодируются как символьная строка '?'. Первая задача состоит в конвертации пропущенных значений в значение NaNs (NaN - Not a Number). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace('?', np.NaN) # заменим '?' на np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее подсчитаем количество пропущенных значений в каждом столбце набора данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Число записей = %d' % (data.shape[0]))\n",
    "print('Число признаков = %d' % (data.shape[1]))\n",
    "\n",
    "print('Число пропущенных значений:')\n",
    "for col in data.columns:\n",
    "    print('\\t%s: %d' % (col,data[col].isna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Среди всех столбцов только столбец 'Bare Nuclei' содержит пропущенные  значения. Заменим пропущенные значения в столбце 'Bare Nuclei' на медиану столбца при помощи метода `fillna()` (значения до и после замены показаны на подмножестве записей)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data['Bare Nuclei']\n",
    "\n",
    "print('До замены отсутствующих значений:')\n",
    "print(data2[20:25])\n",
    "data2 = data2.fillna(data2.median())\n",
    "\n",
    "print('\\nПосле замены отсутствующих значений:')\n",
    "print(data2[20:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вместо замены пропущенных значений можно удалить записи (строки), содержащие пропущенные значения. Для этого можно использовать метод `dropna()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Число записей в исходных данных = %d' % (data.shape[0]))\n",
    "\n",
    "data2 = data.dropna()\n",
    "print('Число записей после удаления отсутствующих значений = %d' % \\\n",
    "      (data2.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбросы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбросами (outliers) называются записи (строки) с характеристиками, которые существенно отличаются от характеристик остальных записей набора данных. \n",
    "\n",
    "Ниже мы изобразим диаграммы размаха (boxplot) столбцов, чтобы найти столбцы таблицы, которые содержат выбросы. Так как столбец 'Bare Nuclei' идентифицирован Pandas как текстовый (из-за пропущенных  значений, представленных строками '?'), нам придется конвертировать столбец в числовые значения при помощи функции `pd.to_numeric()` или метода `astype()` для того, чтобы использовать диаграмму размаха. В противном случае столбец не будет отображаться на рисунке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['Bare Nuclei']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.drop(['Class'],axis=1)\n",
    "data2['Bare Nuclei'] = pd.to_numeric(data2['Bare Nuclei'])\n",
    "data2.boxplot(figsize=(20,3),rot=45); # rot=45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Диаграммы размаха показывают, что только пять столбцов (Marginal Adhesion, Single Epithetial Cell Size, Bland Cromatin, Normal Nucleoli, Mitoses) содержат ненормально большие значения. \n",
    "\n",
    "Чтобы убрать выбросы, можно посчитать стандартизованную оценку (Z-score) для каждого признака и убрать записи, содержащие атрибуты с ненормально высоким или низким Z-score (например, $Z>3$ или $Z<-3$). Для нормального распределения вероятность отклонения случайной величины от своего математического ожидания более чем на три стандартных отклонения практически равна нулю (правило трех сигм).\n",
    "\n",
    "Следующий код показывает результаты стандартизации стоблцов с данными. Отсутствующие значения (NaN) не затрагиваются процессом стандартизации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = (data2-data2.mean())/data2.std()\n",
    "Z[20:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующий код показывает результаты удаления строк, для которых $Z>3$ или $Z<-3$. Число 9 соответствует количеству столбцов в Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Число записей до удаления выбросов = %d' % (Z.shape[0]))\n",
    "\n",
    "Z2 = Z.loc[((Z >= -3).sum(axis=1)==9) & ((Z <= 3).sum(axis=1)==9),:]\n",
    "print('Число записей после удаления выбросов = %d' % (Z2.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дублирующиеся данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые наборы данных, особенно полученные слиянием данных из нескольких источников, могут содержать дублирующиеся записи. \n",
    "\n",
    "Создадим синтетический датафрейм с дублирующимися строками:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dup = pd.DataFrame({'c1': ['x'] * 3 + ['y'] * 4, \n",
    "                         'c2': [1, 1, 2, 3, 3, 4, 4]})\n",
    "data_dup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим, какие строки являются дублирующимися, то есть какие строки уже ранее встречались в датафрейме:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dup.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим дублирующиеся записи (строки), каждый раз оставляя первую из дублирующихся записей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dup.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим дублирующиеся записи, каждый раз оставляя последнюю из дублирующихся записей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dup.drop_duplicates(keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим новый столбец и отследим дублирующиеся записи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dup['c3'] = range(7)\n",
    "data_dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dup.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь дублирующихся записей вообще нет, так как в столбце `c3` все значения отличаются. Но можно найти и удалить дублирующиеся записи с учетом значений в столбцах `c1` и `c2`, результаты будут выглядеть так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dup.drop_duplicates(['c1', 'c2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчитаем дублирующиеся записи в наборе данных с информацией о пациентах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = data.duplicated()\n",
    "print('Число дублирующихся записей = %d' % (dups.sum()))\n",
    "data.loc[[11,28]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `duplicated()` возвращает булевский массив, который показывает является ли запись дубликатом какой-либо предыдущей записи в таблице. Результат означает, что в наборе данных пациентов с раком груди имеется 236 дублирующихся записей. Например, строка с индексом 11 имеет те же значения признаков, что и строка с индексом 28. \n",
    "\n",
    "Хотя дублирующиеся записи могут соответствовать данным различных пациентов, допустим, что дублирующиеся записи  соответствуют одному и тому же пациенту и удалим их:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Число записей до удаления дубликатов = %d' % (data.shape[0]))\n",
    "data2 = data.drop_duplicates()\n",
    "print('Число записей после удаления дубликатов = %d' % (data2.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Трансформация (преобразование) данных\n",
    "\n",
    "#### Замена значений\n",
    "\n",
    "1) метод `map()`\n",
    "\n",
    "Создадим два объекта Series для иллюстрации процесса сопоставления значений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series({\"r1\": 1, \"r2\": 2, \"r3\": 3})\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.Series({1: \"a\", 2: \"b\", 3: \"c\"})\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сопоставим индексам объекта  `x` значения объекта `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.map(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если между значением объекта `y` и индексной меткой объекта `x` не будет найдено соответствие, будет выдано значение NaN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.loc[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.map(y.loc[1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) метод `replace()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.replace(2,2022) # заменяем 2 на 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.replace([1,3],[111,333]) # заменяем значения 1, 3 на 111, 333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.replace({1:2021, 2:2022}) # замена по словарю"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Применение функций к данным\n",
    "\n",
    "1) метод `apply()` применяется к строкам/столбцам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(np.arange(0, 5))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.apply(lambda v: v * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим датафрейм, чтобы проиллюстрировать применение операции суммирования к каждому столбцу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.arange(12).reshape(4, 3), \n",
    "                  columns=['a', 'b', 'c'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислим сумму элементов в каждом столбце:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(lambda col: col.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислим сумму элементов в каждой строке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(lambda row: row.sum(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим столбец `d` путем умножения столбцов `a` и `b`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['d'] = df.apply(lambda row: row.a * row.b, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь получим столбец `r` путем сложения столбцов `c` и `d`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['r'] = df.apply(lambda row: row.c + row.d, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) метод `applymap()` применяется ко всем элементам датафрейма:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.applymap(lambda x: np.exp(x)/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стандартизация и нормализация признака"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стандартизацией случайной величины $X$ называют ее линейное преобразование, приводящее к случайной величине с математическим ожиданием 0 и стандартным отклонением 1:\n",
    "\n",
    "$\\tilde{X}=\\frac{X-\\mathbb{E}\\left[X\\right]}{\\sqrt{\\mathbb{V}\\left[X\\right]}},$\n",
    "\n",
    "где $\\mathbb{E}$ – операция вычисления математического ожидания, $\\mathbb{V}$ – операция вычисления дисперсии. \n",
    "\n",
    "Стандартизация набора данных является существенным условием для применения многих алгоритмов машинного обучения, а именно, алгоритмы дают приемлемый результат, только если отдельные признаки распределены примерно как стандартные нормальные величины (с нулевым матожиданием и единичной дисперсией).\n",
    "\n",
    "Для стандартизации признаков набора данных может быть использована функция `scale()` из модуля `preprocessing`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X = np.array([[ 1., -1.,  2.],\n",
    "              [ 2.,  0.,  0.],\n",
    "              [ 0.,  1., -1.]])\n",
    "X_scaled = preprocessing.scale(X)\n",
    "print(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стандартизованный набор данных имеет признаки с нулевыми средними и единичной дисперсией:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_scaled.mean(axis=0))\n",
    "print(X_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также модуль preprocessing содержит класс `StandardScaler`, который позволяет сохранить математическое ожидание и стандартное отклонение обучающей выборки и затем применять то же преобразование к другим выборкам. \n",
    "\n",
    "Альтернативным вариантом нормализации является масштабирование признака между заданным минимальным и максимальным значениями (нормализация). Этот эффект может быть достигнут при помощи функций `MinMaxScaler()` или `MaxAbsScaler()`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[ 1., -1.,  2.],\n",
    "              [ 2.,  0.,  0.],\n",
    "              [ 0.,  1., -1.]])\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_minmax = min_max_scaler.fit_transform(X)\n",
    "X_minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция `MaxAbsScaler()` используется аналогично, но масштабирует данные в диапазон $\\left[-1,\\,1\\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Семплирование данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Семплирование (от англ. sample — выборка), или методы управления выборкой данных, – это подход, направленный на:\n",
    "\n",
    "1. сокращение объема данных для анализа данных и масштабирования алгоритмов для приложений с большими данными\n",
    "\n",
    "2. количественную оценку неопределенностей из-за различного распределения данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существуют различные методы выборки данных, такие как выборка без замены, когда каждый выбранный экземпляр удаляется из набора данных, и выборка с заменой, где каждый выбранный экземпляр не удаляется, что позволяет выбирать его более одного раза.\n",
    "\n",
    "В примере ниже мы применим выборку с заменой и без замены с набору данных пациентов с раком груди.\n",
    "\n",
    "Выведем первые пять записей набора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее данные для выборки размера 3 (без замены) выбираются случайным образом из исходных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data.sample(n=3)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В следующем примере мы случайным образом выбираем 1% данных (без замены) и выводим выбранные записи. Параметр `random_state` задает начальное значение для генератора случайных чисел. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data.sample(frac=0.01, random_state=1)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, выполним выборку с заменой размером, равным 1% всех данных. Можно увидеть повторяющиеся записи в выборке, если увеличить ее размеры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data.sample(frac=0.01, replace=True, random_state=1)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дискретизация данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дискретизация – это этап препроцессинга, который часто используется при преобразовании непрерывного признака в категориальный. \n",
    "\n",
    "Пример ниже иллюстрирует два простых, но часто применяемых метода дискретизации (равной ширины,  равных частот) для признака 'Clump Thickness' набора данных пациентов с раком груди.\n",
    "\n",
    "Вначале нарисуем гистограмму, которая показывает распределение значений признака. Метод `value_counts()` также может быть использован, чтобы подсчитать частоты каждого значения признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Clump Thickness'].hist(bins=10)\n",
    "data['Clump Thickness'].value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При использовании метода равной ширины можно задействовать функцию `cut()`, чтобы дискретизировать признак в 4 бина, имеющих равную ширину. Метод `value_counts()` может быть использован для определения числа записей в каждом из бинов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = pd.cut(data['Clump Thickness'],4)\n",
    "bins.value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При использовании метода равных частот можно задействовать функцию `qcut()` для разделения значений признака на 4 бина, имеющих примерно равное число записей. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = pd.qcut(data['Clump Thickness'],4)\n",
    "bins.value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дискретизация также возможна при помощи средств библиотеки scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кодирование категориальных признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В большинстве наборов данных присутствуют категориальные признаки, которые содержат значения в текстовом формате. Примерами являются цвета (“Red”, “Green”, “Yellow”, “Blue”), размеры (“Small”, “Medium”, “Large”, “Extra Large”), географические обозначения (страны, города и т.п.). Независимо от назначения категориальных признаков возникает вопрос, как использовать категориальные признаки при анализе данных. Многие алгоритмы машинного обучения поддерживают категориальные значения без необходимости каких-либо манипуляций с данными, однако есть и такие алгоритмы, которые требуют преобразования текстовых значений в числовые для дальнейшей обработки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Набор данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим набор данных Automobile из репозитария UCI, содержащий как категориальные, так и непрерывные признаки. \n",
    "\n",
    "Импортируем данные, выполняя попутно обработку пропущенных значений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определяем метки столбцов\n",
    "headers = [\"symboling\", \"normalized_losses\", \"make\", \"fuel_type\", \"aspiration\",\n",
    "    \"num_doors\", \"body_style\", \"drive_wheels\", \"engine_location\",\n",
    "    \"wheel_base\", \"length\", \"width\", \"height\", \"curb_weight\",\n",
    "    \"engine_type\", \"num_cylinders\", \"engine_size\", \"fuel_system\",\n",
    "    \"bore\", \"stroke\", \"compression_ratio\", \"horsepower\", \"peak_rpm\",\n",
    "    \"city_mpg\", \"highway_mpg\", \"price\"]\n",
    "# считываем CSV файл и конвертируем значения \"?\" в NaN\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/\"+\\\n",
    "      \"autos/imports-85.data\"\n",
    "df = pd.read_csv(url, header=None, names=headers, na_values=\"?\" )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы понять, с какими типами данным мы имеем дело, рассмотрим свойство"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как нас интересуют только категориальные признаки, оставим в наборе столбцы с типом `“object”`. Pandas содержит удобный метод `select_dtypes()`, который можно использовать, чтобы оставить в наборе только столбцы с типом `“object”` (категориальные признаки):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df = df.select_dtypes(include=['object']).copy()\n",
    "obj_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построенный набор содержит несколько строк с пропущенными значениями, которые нужно заполнить:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df[obj_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В наборе наиболее часто встречается значение \"four\" (4 двери):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df[\"num_doors\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для простоты заполним пропущенные значения этим значением:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df = obj_df.fillna({\"num_doors\": \"four\"})\n",
    "obj_df[obj_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь набор не содержит пропущенных значений и мы можем приступить к кодированию категориальных значений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Замена значений признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В двух столбцах набора данных текстовые значения представляют собой числа, а именно, число цилиндров в двигателе и число дверей в автомобиле. \n",
    "\n",
    "Признак \"num_cylinders\" принимает 7 значений, которые легко преобразуются в целые числа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df[\"num_cylinders\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `replace()` из Pandas имеет множество опций, в частности, опцию словаря, содержащего названия столбцов и словари для отображения старых значений в новые значения.\n",
    "\n",
    "Словарь для преобразования признаков \"num_doors\" и \"num_cylinders\" в числовые значения задается следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_nums = {\"num_doors\": {\"four\": 4, \"two\": 2},\n",
    "    \"num_cylinders\": {\"four\": 4, \"six\": 6, \"five\": 5, \"eight\": 8,\n",
    "    \"two\": 2, \"twelve\": 12, \"three\":3 }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для преобразования признаков в числовые значения выполним код: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df.replace(cleanup_nums, inplace=True)\n",
    "obj_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas автоматически преобразует тип признаков в числовой (int64):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описанный подход работает в тех случаях, когда имеется понятный способ интерпретации текстовых значений как числовых."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Кодирование меток"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кодирование меток (label encoding) – это способ конвертации значений в столбцах в числа.\n",
    "\n",
    "Например, столбец body_style содержит 5 различных значений. Можно закодировать их так: \n",
    "\n",
    "    convertible -> 0  \n",
    "    hardtop -> 1  \n",
    "    hatchback -> 2  \n",
    "    sedan -> 3  \n",
    "    wagon -> 4  \n",
    "\n",
    "Можно использовать Pandas, чтобы преобразовать столбец в категорию (категория – это тип данных в Pandas, принимающий несколько значений), а потом использовать значения категории для кодирования меток:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df[\"body_style\"] = obj_df[\"body_style\"].astype('category')\n",
    "obj_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее можно присвоить закодированные значения признака новому столбцу \"body_style_cat\" используя свойство `cat.codes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df[\"body_style_cat\"] = obj_df[\"body_style\"].cat.codes\n",
    "obj_df[['body_style','body_style_cat']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Особенностью этого подхода является то, что появляется возможность использовать преимущества категорий Pandas (компактность данных, возможность упорядочения, поддержка визуализации), при этом категории могут быть легко конвертированы в числовые значения для дальнейшего анализа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Прямое кодирование (One Hot Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кодирование меток имеет преимущество в виде простоты реализации и недостаток, состоящий в том, что числовое значение может быть некорректно интерпретировано алгоритмами машинного обучения. Например, значение 0, очевидно, меньше значения 2, но соответствует ли эта зависимость реальной ситуации для текстовых значений? \n",
    "\n",
    "Альтернативный подход (прямое кодирование) состоит в том, чтобы конвертировать каждую категорию в новый столбец, принимающий значения 1 или 0 (True/False). Преимуществом этого подхода является то, что между категориальными значениями не устанавливаются несуществующие связи, а недостатком – что в наборе данных появляются дополнительные столбцы. \n",
    "\n",
    "Pandas поддерживает этот подход в функции `get_dummies()`, которая создает новые столбцы вида “столбец_значение”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим пример для столбца drive_wheels со значениями 4wd , fwd, rwd. Используя `get_dummies()` мы конвертируем этот столбец в три столбца со значениями 1 или 0, соответствующими правильному значению исходного признака (столбца):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(obj_df, columns=[\"drive_wheels\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Столбец \"drive_wheels\" пропал, при этом новый набор данных содержит три новых столбца:\n",
    "\n",
    "• drive_wheels_4wd  \n",
    "• drive_wheels_rwd  \n",
    "• drive_wheels_fwd  \n",
    "\n",
    "В функцию `get_dummies()` можно передать несколько столбцов с категориальными признаками, а также передать префиксы для именования новых столбцов с целью упростить последующий анализ данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(obj_df, columns=[\"body_style\", \"drive_wheels\"], \\\n",
    "               prefix=[\"body\", \"drive\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прямое кодирование является очень полезным инструментом, однако может приводить к резкому увеличению числа столбцов в наборе, если категориальные признаки имеют большое число различных значений. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Двоичное кодирование, управляемое пользователем "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В зависимости от особенностей набора данных можно использовать различные комбинации кодирования меток и прямого кодирования, которые в наибольшей степени соответствуют целям дальнейшего анализа. \n",
    "\n",
    "Для иллюстрации двоичного кодирования, управляемого пользователем (custom binary encoding), рассмотрим следующий пример. В наборе данных имеется столбец engine_type (тип двигателя), который содержит несколько различных значений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df[\"engine_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим, что требуется выделить в отдельную группу все двигатели с верхней камерой (Overhead Cam или OHC). Другими словами, различные версии OHC эквивалентны для анализа. В это случае можно использовать свойство `str` и функцию `np.where`, чтобы создать новый столбец как индикатор того, что двигатель автомобиля имеет тип OHC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df[\"OHC_Code\"] = np.where(obj_df[\"engine_type\"].str.contains(\"ohc\"), 1, 0)\n",
    "obj_df[\"OHC_Code\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате получаем набор данных, включающий столбец OHC_Code (показываем в наборе только три столбца):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df[[\"make\", \"engine_type\", \"OHC_Code\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный подход является по-настоящему полезным, если имеется возможность консолидировать бинарные значения (да/нет) в новом столбце. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Возможности кодирования в библиотеке Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека scikit-learn также содержит функцонал для кодирования текстовых признаков.\n",
    "\n",
    "Например, чтобы кодировать метки для производителей автомобиля, используем объект `LabelEncoder` и метод `fit_transform()` для столбца с данными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb_make = LabelEncoder()\n",
    "obj_df[\"make_code\"] = lb_make.fit_transform(obj_df[\"make\"])\n",
    "obj_df[[\"make\", \"make_code\"]].head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn также поддерживает бинарное кодирование при помощи объекта `LabelBinarizer`. Можно использовать процедуру, аналогичную приведенной выше, чтобы преобразовать данные, но требуются некоторые дополнительные шаги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb_style = LabelBinarizer()\n",
    "lb_results = lb_style.fit_transform(obj_df[\"body_style\"])\n",
    "pd.DataFrame(lb_results, columns=lb_style.classes_).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На следующем шаге нужно включить эти данные в исходный набор данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отбор признаков "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Набор признаков, используемых для обучения модели, оказывает значительное влияние на качество результатов. Присутствие в наборе данных малоинформативных признаков приводит к снижению точности многих моделей, особенно моделей регрессии.\n",
    "\n",
    "Отбор признаков (feature selection) – это процесс выбора признаков, обеспечивающий более высокое качество модели машинного обучения.\n",
    "\n",
    "Отбор признаков перед построением модели обеспечивает следующие преимущества:\n",
    "\n",
    "• Уменьшение переобучения. Чем меньше избыточных данных, тем меньше возможностей для модели принимать решения на основе «шума».\n",
    "\n",
    "• Повышение точности. Чем меньше противоречивых данных, тем выше точность.\n",
    "\n",
    "• Сокращение времени обучения. Чем меньше данных, тем быстрее обучается модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем работать с набором данных, содержащим информацию о качество вина. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаление признаков с низкой дисперсией"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простейшим подходом к отбору признаков является исключение признаков с низкой дисперсией. Если дисперсия признака равна нулю, то признак для всех записей имеет одно и то же значение и может не приниматься во внимание при анализе данных. Если дисперсия признака близка к нулю, то признак принимает значения, близкие к некоторому (среднему) значению и, скорее всего, является несущественным. \n",
    "\n",
    "В качестве примера рассмотрим гипотетический набор данных с булевыми признаками и допустим, что мы хотим удалить все признаки, в которых нули или единицы составляют более чем 80% значений. Булевы признаки могут быть интепретированы как случайные величины с распределением Бернулли, имеющие дисперсию\n",
    "\n",
    "$V\\left[X\\right]=p\\,\\left(1-p\\right),$\n",
    "\n",
    "поэтому при отборе признаков можем использовать пороговое значение $0.8\\,(1-0.8)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X = [[0, 0, 1], \n",
    "     [0, 1, 0], \n",
    "     [1, 0, 0], \n",
    "     [0, 1, 1], \n",
    "     [0, 1, 0], \n",
    "     [0, 1, 1]]\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "sel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и ожидалось, метод `VarianceThreshold` удалил первый столбец, для которого вероятность нулевого значения $p=\\frac{5}{6}>0.8$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Одномерный отбор признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Признаки, имеющие наиболее выраженную взаимосвязь с целевой переменной, могут быть отобраны с помощью статистических критериев. Библиотека scikit-learn содержит класс `SelectKBest`, реализующий одномерный отбор признаков (univariate feature selection). Этот класс можно применять совместно с различными статистическими критериями для отбора заданного количества признаков.\n",
    "\n",
    "В примере ниже используется критерий хи-квадрат (chi-squared test) для неотрицательных признаков, чтобы отобрать 4 лучших признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отбор признаков при помощи одномерных статистических тестов \n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "\n",
    "# загрузка данных - качество вина\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/\"+\\\n",
    "      \"wine-quality/winequality-red.csv\"\n",
    "df = pd.read_csv(url,sep=\";\")\n",
    "print(\"\\nИсходный набор данных:\\n\",df.head())\n",
    "array = df.values\n",
    "X = array[:,0:11] # входные переменные (11 признаков)\n",
    "Y = array[:,11]   # выходная переменная - качество (оценка между 0 и 10)\n",
    "\n",
    "# отбор признаков\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "fit = test.fit(X, Y)\n",
    "\n",
    "# оценки признаков\n",
    "print(\"\\nОценки признаков:\\n\",fit.scores_)\n",
    "\n",
    "cols = test.get_support(indices=True)\n",
    "df_new = df.iloc[:,cols]\n",
    "print(\"\\nОтобранные признаки:\\n\",df_new.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим оценки для каждого признака и 4 отобранных признака (с наивысшими оценками): volatile acidity, free sulfur dioxide, total sulfur dioxide и alcohol.\n",
    "\n",
    "Если выходная (зависимая) переменная представляет собой класс, то можно использовать статистические критерии `chi2` или `f_classif`. Если выходная (зависимая) переменная представляет собой признак, принимающий непрерывные значения, то следует использовать статистический критерий `f_regression`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Отбор на основе важности признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ансамблевые алгоритмы на основе деревьев решений, такие как случайный лес (random forest), позволяют оценить важность признаков.\n",
    "\n",
    "В представленном ниже примере мы обучаем классификатор `ExtraTreesClassifier`, чтобы с его помощью определить важность признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# важность признаков с классификатором Extra Trees\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# загрузка данных - качество вина\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/\"+\\\n",
    "      \"wine-quality/winequality-red.csv\"\n",
    "df = pd.read_csv(url, sep=\";\")\n",
    "\n",
    "array = df.values\n",
    "X = array[:,0:11] # входные переменные (11 признаков)\n",
    "Y = array[:,11]   # выходная переменная - качество (оценка между 0 и 10)\n",
    "\n",
    "# отбор признаков\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, Y)\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили оценки для каждого признака. Чем больше значение оценки, тем важнее признак. Таким образом, согласно данному методу отбора, двумя наиболее важными признаками являются два последних признака (total sulfur dioxide и sulphates)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод главных компонент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод главных компонент (principal component analysis, PCA) позволяет уменьшить размерность данных с помощью преобразования на основе линейной алгебры. Пользователь может задать требуемое количество измерений (главных компонент) в результирующих данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем набор данных \"Ирисы\" и сократим его размерность до двух:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/\"+\\\n",
    "      \"iris/iris.data\"\n",
    "\n",
    "# считываем данные в объект data frame\n",
    "my_data = pd.read_csv( url, header=None, usecols=(0,1,2,3) )\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "pcad = pca.fit_transform(my_data) # numpy array\n",
    "\n",
    "print( \"*** Первые 5 строк данных:\" )\n",
    "for x in range(0,5):\n",
    "  print( pcad[x] )  \n",
    "\n",
    "print( \"*** Дисперсии компонент:\\n\", pca.explained_variance_ratio_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим уровень объясняемой дисперсии для различных значений параметра `n_components`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(1,5):\n",
    "  pca = PCA( n_components = r )\n",
    "  pca.fit( my_data )\n",
    "  print( \"r =\",r,\"\\tДисперсия =\",\n",
    "        sum(pca.explained_variance_ratio_)*100,\"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В примере ниже выделим 3 главных компоненты с помощью PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка данных - качество красного вина\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/\"+\\\n",
    "      \"wine-quality/winequality-red.csv\"\n",
    "df = pd.read_csv(url, sep=\";\")\n",
    "\n",
    "array = df.values\n",
    "X = array[:,0:11] # входные переменные (11 признаков)\n",
    "\n",
    "# главные компоненты\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X)\n",
    "features = fit.transform(X)\n",
    "\n",
    "# результаты\n",
    "print(\"Объясняемая дисперсия:\", sum(fit.explained_variance_ratio_)*100)\n",
    "print(features[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат преобразования (3 главных компоненты) совсем не похож на исходные данные и содержит отрицательные значения.\n",
    "\n",
    "## Вычисления с данными датафрейма\n",
    "\n",
    "Чтобы получить сводку статистик для числовых столбцов датафрейма, можно воспользоваться методом `describe()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно вычислить сводку статистик для отдельного столбца, например, `fixed acidity`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fixed acidity'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полезная информация о датафрейме также может быть получена при помощи метода `info()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для нечислового столбца получаем при помощи `describe()` такую статистику:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df.make.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно также вычислить нормализованные частоты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df.make.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы определить минимальные значения для столбцов числового датафрейма, можно воспользоваться методом `min()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Индексы для минимальных значений определяются при помощи метода `idxmin()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.idxmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для максимальных значений используются методы `max()` и `idxmax()`. \n",
    "\n",
    "Для вычисления минимальных/максимальных значений по строкам применяется ключ `axis=1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для вычисления средних значений и медиан используются методы `mean()` и `median()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean(axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для вычисления дисперсии и стандартного отклонения используем методы `var()` и `std()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.std(axis=1).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для вычисления ковариации и корреляции между двумя столбцами можно использовать методы `cov()` и `corr()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fixed acidity'].cov(df['volatile acidity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.alcohol.corr(df.quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно вычислить матрицы ковариаций и корреляций признаков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы отобрать из датафрейма со столбцами разных типов только числовые столбцы, можно использовать метод `select_dtypes()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df.select_dtypes(include=np.number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация данных\n",
    "\n",
    "Основной библиотекой визуализации данных является библиотека Matplotlib. Библиотека Pandas имеет встроенный интерфейс для обращения к Matplotlib.\n",
    "\n",
    "Загрузим с локального диска два датасета с данными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500 = pd.read_csv(\"sp500.csv\",\n",
    "                    index_col='Symbol',\n",
    "                    usecols=['Symbol', 'Sector', 'Price',\n",
    "                             'Book Value', 'Market Cap',\n",
    "                             'Dividend Yield'])\n",
    "sp500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omh = pd.read_csv('omh.csv', parse_dates=['Date'])\n",
    "omh.set_index('Date', inplace=True)\n",
    "omh.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуем график цены акций Microsoft:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omh.MSFT.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цены акций Microsoft и Apple на одном графике:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omh.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как цены акций имеют разный масштаб, можно выполнить нормализацию цен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omh_copy =  (omh - omh.mean())/omh.std()\n",
    "omh_copy.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно задать желаемые размеры графика:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omh_copy.plot(figsize=(10, 5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заголовок может быть задан при помощи параметра `title` метода `plot()`, а подписи осей `x` и `y` могут быть заданы обращением к Matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "omh_copy.plot(title='Цены акций после нормировки', figsize=(10, 5))\n",
    "plt.xlabel('Дата')\n",
    "plt.ylabel('Цена');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно изменить элементы легенды, соответствующие именам столбцов датафрейма:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = omh_copy.plot(figsize=(10, 5))\n",
    "ax.legend(['Microsoft', 'Apple']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно изменить расположение легенды:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = omh_copy.plot(figsize=(10, 5))\n",
    "ax.legend(loc='upper center');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно вообще отключить легенду:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omh_copy.plot(figsize=(10, 5), legend=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменим цвета линий графика:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omh_copy.plot(style={'MSFT': 'b', 'AAPL': 'g'}); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем различные стили линий и увеличим толщину линий:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omh_copy.plot(style={'MSFT': 'b--', 'AAPL': 'g:'}, lw=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно добавить маркеры линий:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omh_copy.plot(style={'MSFT': 'b--^', 'AAPL': 'g:o'});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из Pandas можно нарисовать различные виды графиков.\n",
    "\n",
    "* столбчатая диаграмма (bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sp500.Sector.value_counts()\n",
    "s.plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сократим данные для визуализации, отбросив, в частности, малочисленные сектора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_sectors = s[-4:].index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислим квантили 95%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500.quantile(0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим усеченную копию данных так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (~sp500.Sector.isin(small_sectors)) \\\n",
    "    & (sp500.Price < 184) \\\n",
    "    & (sp500['Book Value'] < 66) \\\n",
    "    & (sp500['Market Cap'] < 134) \\\n",
    "    & (sp500['Dividend Yield'] < 4.5)\n",
    "\n",
    "sp500_cut = sp500.loc[idx].copy()\n",
    "sp500_cut.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим столбчатую диаграмму для средних и медиан:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sp500_cut.groupby('Sector').Price.agg(['mean', 'median'])\n",
    "df.plot(kind='bar', figsize=(10, 5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример вертикально состыкованной столбчатой диаграммы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='bar', stacked=True, figsize=(10, 5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Горизонтально состыкованная столбчатая диаграмма имеет вид:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='barh', stacked=True, figsize=(10, 5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* гистограмма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_cut.Price.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_cut.Price.hist(bins = 50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для датафрейма получается 4 гистограммы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_cut.hist(figsize=(12,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно наложить несколько гистограмм при помощи модуля pyplot: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(sp500_cut.Price, alpha=0.75, label='Price')\n",
    "plt.hist(sp500_cut['Book Value'], alpha=0.75, label='Book Value')\n",
    "plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sp500_cut.Price\n",
    "s.hist(density=True)\n",
    "s.plot(kind='kde', lw=20, alpha=0.75, figsize=(10,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* диаграмма размаха (boxplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_cut[['Price','Book Value']].boxplot(figsize = (14, 7));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* диаграмма рассеяния (scatter plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_cut.plot(kind='scatter',x='Price',y='Book Value',figsize=(10,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(sp500_cut, alpha=0.4, figsize=(9, 9), diagonal='kde');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* тепловая карта (heat map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = sp500_cut.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(corr_matrix, cmap='Greens')\n",
    "plt.colorbar()  # добавим шкалу интенсивности цвета\n",
    "\n",
    "plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n",
    "plt.yticks(range(len(corr_matrix)), corr_matrix.index);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведем пример визуализации набора данных со сниженной размерностью (разными цветами) при помощи Matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt( \"iris.csv\", delimiter=\",\", usecols=(0,1,2,3) ) \n",
    "target = np.genfromtxt( \"iris.csv\", delimiter=\",\", usecols=(4), dtype=str )\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pcad = pca.fit_transform( data )\n",
    "\n",
    "plt.figure( figsize=(8, 6), dpi=200 )\n",
    "plt.plot(pcad[target==\"Iris-setosa\",0],\n",
    "         pcad[target==\"Iris-setosa\",1],\"bo\") \n",
    "plt.plot(pcad[target==\"Iris-versicolor\",0],\n",
    "         pcad[target==\"Iris-versicolor\",1],\"r.\") \n",
    "plt.plot(pcad[target==\"Iris-virginica\",0],\n",
    "         pcad[target==\"Iris-virginica\",1],\"g+\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание на лабораторную работу №1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание (10 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для закрепленного за Вами варианта лабораторной работы:\n",
    "\n",
    "1.\tИспользуя функционал библиотеки Pandas, cчитайте заданный набор данных из репозитария UCI. Набор данных задан ссылкой на страницу набора данных и названием файла с данными, который доступен из папки с данными (data folder). \n",
    "\n",
    "2.\tПроведите исследование набора данных, выявляя числовые признаки. Если какие-то из числовых признаков были неправильно классифицированы, то преобразуйте их в числовые. Если в наборе для числовых признаков присутствуют пропущенные значения ('?'), то заполните их медианными значениями признаков.\n",
    "\n",
    "3.\tОпределите столбец, содержащий метку класса (отклик). Если столбец, содержащий метку класса (отклик), принимает более 10 различных значений, то выполните дискретизацию этого столбца, перейдя к 4-5 диапазонам значений. \n",
    "\n",
    "4.\tПри помощи класса `SelectKBest` библиотеки scikit-learn найдите в наборе два признака, имеющих наиболее выраженную взаимосвязь с (дискретизированным) столбцом с меткой класса (откликом). Используйте для параметра `score_func` значения `chi2` или `f_classif`. \n",
    "\n",
    "5.\tДля найденных признаков и (дискретизированного) столбца с меткой класса (откликом) вычислите матрицу корреляций и визуализируйте ее в виде тепловой карты (heat map). \n",
    "\n",
    "6.\tВизуализируйте набор данных в виде диаграммы рассеяния на плоскости с координатами, соответствующими найденным признакам, отображая точки различных классов разными цветами. Подпишите оси и рисунок, создайте легенду набора данных.\n",
    "\n",
    "7.\tОставляя в наборе данных только числовые признаки, найдите и выведите на экран размерность метода главных компонент (параметр `n_components`), для которой доля объясняемой дисперсии будет не менее 97.5%.\n",
    "\n",
    "8.\tПользуясь методом главных компонент (PCA), снизьте размерность набора данных до двух признаков и изобразите полученный набор данных в виде диаграммы рассеяния на плоскости, образованной двумя полученными признаками, отображая точки различных классов разными цветами. Подпишите оси и рисунок, создайте легенду набора данных.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
